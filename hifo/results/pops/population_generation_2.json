[
     {
          "algorithm": "A hybrid algorithm that integrates adaptive candidate selection, Monte Carlo simulation for look-ahead evaluation, and MST-based global cost estimation with dynamically tuned weights favoring exploration in early stages and exploitation in later stages.}\n\n```python\nimport numpy as np\n\ndef select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):\n    def mst_heuristic(nodes, dist_matrix):\n        if len(nodes) == 0:\n            return 0\n        edges = [(dist_matrix[i][j], i, j) for i in nodes for j in nodes if i < j]\n        edges.sort(key=lambda x: x[0])\n        parent = {node: node for node in nodes",
          "code": "import numpy as np\n\ndef select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):\n    def mst_heuristic(nodes, dist_matrix):\n        if len(nodes) == 0:\n            return 0\n        edges = [(dist_matrix[i][j], i, j) for i in nodes for j in nodes if i < j]\n        edges.sort(key=lambda x: x[0])\n        parent = {node: node for node in nodes}\n        \n        def find(u):\n            while parent[u] != u:\n                parent[u] = parent[parent[u]]\n                u = parent[u]\n            return u\n        \n        mst_cost = 0\n        for cost, u, v in edges:\n            if find(u) != find(v):\n                parent[find(v)] = find(u)\n                mst_cost += cost\n        return mst_cost\n    \n    def monte_carlo_simulation(node, remaining_nodes, dist_matrix, simulations=15):\n        total_cost = 0\n        for _ in range(simulations):\n            temp_remaining = remaining_nodes[:]\n            current = node\n            cost = 0\n            while temp_remaining:\n                next_node = min(temp_remaining, key=lambda x: dist_matrix[current][x])\n                cost += dist_matrix[current][next_node]\n                current = next_node\n                temp_remaining.remove(next_node)\n            cost += dist_matrix[current][destination_node]\n            total_cost += cost\n        return total_cost / simulations\n    \n    # Adaptive candidate set generation\n    candidate_set_size = max(5, len(unvisited_nodes) // 3)\n    candidate_set = sorted(unvisited_nodes, key=lambda x: distance_matrix[current_node][x])[:candidate_set_size]\n    \n    # Hybrid scoring mechanism with Monte Carlo look-ahead and dynamic weighting\n    scores = []\n    exploration_weight = 0.6 if len(unvisited_nodes) > len(distance_matrix) // 2 else 0.3\n    exploitation_weight = 1.0 - exploration_weight\n    for node in candidate_set:\n        remaining_after_choice = [n for n in unvisited_nodes if n != node]\n        future_cost_mst = mst_heuristic(remaining_after_choice, distance_matrix)\n        future_cost_mc = monte_carlo_simulation(node, remaining_after_choice, distance_matrix)\n        score = (distance_matrix[current_node][node] + \n                 exploration_weight * future_cost_mst + \n                 exploitation_weight * future_cost_mc)\n        scores.append((node, score))\n    \n    # Select the node with the lowest score\n    next_node = min(scores, key=lambda x: x[1])[0]\n    return next_node",
          "objective": 5.94059,
          "other_inf": null,
          "metadata": {
               "operator": "m2",
               "wisdom_tips": [
                    "Design adaptive hybrid meta-heuristics synergistically fusing multiple search paradigms and dynamically tune operator parameters based on search stage or problem features",
                    "Employ machine learning or pattern recognition to mine deep problem structures and optimal solution patterns then use learned insights to intelligently bias towards promising search regions or constructive choices",
                    "Explore objective function engineering by introducing auxiliary or surrogate objectives or by dynamically adjusting weights to reshape the search landscape aiding escape from local optima or guiding diverse exploration"
               ],
               "attention_focus": "balancing local optimality with global search strategies",
               "generation_tendency": "balanced_search",
               "timestamp": 1748448251.6876318
          }
     },
     {
          "algorithm": "A hybrid algorithm that integrates adaptive candidate selection, Monte Carlo simulation for look-ahead evaluation, and MST-based global cost estimation to iteratively choose the next node while dynamically balancing exploration and exploitation.}\n\n```python\nimport numpy as np\n\ndef select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):\n    def mst_heuristic(nodes, dist_matrix):\n        if len(nodes) == 0:\n            return 0\n        edges = [(dist_matrix[i][j], i, j) for i in nodes for j in nodes if i < j]\n        edges.sort(key=lambda x: x[0])\n        parent = {node: node for node in nodes",
          "code": "import numpy as np\n\ndef select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):\n    def mst_heuristic(nodes, dist_matrix):\n        if len(nodes) == 0:\n            return 0\n        edges = [(dist_matrix[i][j], i, j) for i in nodes for j in nodes if i < j]\n        edges.sort(key=lambda x: x[0])\n        parent = {node: node for node in nodes}\n        \n        def find(u):\n            while parent[u] != u:\n                parent[u] = parent[parent[u]]\n                u = parent[u]\n            return u\n        \n        mst_cost = 0\n        for cost, u, v in edges:\n            if find(u) != find(v):\n                parent[find(v)] = find(u)\n                mst_cost += cost\n        return mst_cost\n    \n    def monte_carlo_simulation(node, remaining_nodes, dist_matrix, simulations=10):\n        total_cost = 0\n        for _ in range(simulations):\n            temp_remaining = remaining_nodes[:]\n            current = node\n            cost = 0\n            while temp_remaining:\n                next_node = min(temp_remaining, key=lambda x: dist_matrix[current][x])\n                cost += dist_matrix[current][next_node]\n                current = next_node\n                temp_remaining.remove(next_node)\n            cost += dist_matrix[current][destination_node]\n            total_cost += cost\n        return total_cost / simulations\n    \n    # Adaptive candidate set generation\n    candidate_set_size = max(3, len(unvisited_nodes) // 2)\n    candidate_set = sorted(unvisited_nodes, key=lambda x: distance_matrix[current_node][x])[:candidate_set_size]\n    \n    # Hybrid scoring mechanism with Monte Carlo look-ahead\n    scores = []\n    for node in candidate_set:\n        remaining_after_choice = [n for n in unvisited_nodes if n != node]\n        future_cost_mst = mst_heuristic(remaining_after_choice, distance_matrix)\n        future_cost_mc = monte_carlo_simulation(node, remaining_after_choice, distance_matrix)\n        score = distance_matrix[current_node][node] + 0.7 * future_cost_mst + 0.3 * future_cost_mc\n        scores.append((node, score))\n    \n    # Select the node with the lowest score\n    next_node = min(scores, key=lambda x: x[1])[0]\n    return next_node",
          "objective": 5.95679,
          "other_inf": null,
          "metadata": {
               "operator": "e1",
               "wisdom_tips": [
                    "Design adaptive hybrid meta-heuristics synergistically fusing multiple search paradigms and dynamically tune operator parameters based on search stage or problem features",
                    "Employ machine learning or pattern recognition to mine deep problem structures and optimal solution patterns then use learned insights to intelligently bias towards promising search regions or constructive choices",
                    "Explore objective function engineering by introducing auxiliary or surrogate objectives or by dynamically adjusting weights to reshape the search landscape aiding escape from local optima or guiding diverse exploration"
               ],
               "attention_focus": "optimizing established successful strategies and patterns",
               "generation_tendency": "focus_exploitation",
               "timestamp": 1748448131.206754
          }
     },
     {
          "algorithm": "The new algorithm integrates adaptive greedy selection, enhanced MST-based cost normalization, dynamic clustering with cluster-aware momentum, and a refined score function incorporating edge-cost variance and look-ahead path consistency.}\n\n```python\nimport numpy as np\nfrom sklearn.cluster import KMeans\n\ndef select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):\n    def mst_heuristic(nodes, dist_matrix):\n        if len(nodes) == 0:\n            return 0\n        edges = [(dist_matrix[i][j], i, j) for i in nodes for j in nodes if i != j]\n        edges.sort(key=lambda x: x[0])\n        parent = {node: node for node in nodes}\n        rank = {node: 0 for node in nodes",
          "code": "import numpy as np\nfrom sklearn.cluster import KMeans\n\ndef select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):\n    def mst_heuristic(nodes, dist_matrix):\n        if len(nodes) == 0:\n            return 0\n        edges = [(dist_matrix[i][j], i, j) for i in nodes for j in nodes if i != j]\n        edges.sort(key=lambda x: x[0])\n        parent = {node: node for node in nodes}\n        rank = {node: 0 for node in nodes}\n        \n        def find(u):\n            while parent[u] != u:\n                parent[u] = parent[parent[u]]\n                u = parent[u]\n            return u\n        \n        def union(u, v):\n            root_u = find(u)\n            root_v = find(v)\n            if root_u != root_v:\n                if rank[root_u] > rank[root_v]:\n                    parent[root_v] = root_u\n                elif rank[root_u] < rank[root_v]:\n                    parent[root_u] = root_v\n                else:\n                    parent[root_v] = root_u\n                    rank[root_u] += 1\n        \n        mst_cost = 0\n        for cost, u, v in edges:\n            if find(u) != find(v):\n                union(u, v)\n                mst_cost += cost\n        return mst_cost\n    \n    def dynamic_clustering(nodes, dist_matrix):\n        coords = np.array([[i, np.mean([dist_matrix[i][j] for j in nodes])] for i in nodes])\n        n_clusters = min(3, len(nodes))\n        kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(coords)\n        clusters = [list(np.array(nodes)[kmeans.labels_ == i]) for i in range(kmeans.n_clusters)]\n        return clusters\n    \n    def refined_score(node, remaining_nodes, clusters, dist_matrix):\n        # Cluster-aware momentum bonus\n        cluster_bonus = max([np.mean([dist_matrix[node][n] for n in cluster]) for cluster in clusters], default=0)\n        # Look-ahead MST cost\n        future_cost = mst_heuristic(remaining_nodes, dist_matrix)\n        # Edge cost variance penalty\n        edge_costs = [dist_matrix[current_node][n] for n in remaining_nodes]\n        cost_variance_penalty = np.std(edge_costs) if edge_costs else 0\n        # Combine into refined score\n        return dist_matrix[current_node][node] + future_cost - cluster_bonus + cost_variance_penalty\n    \n    # Adaptive candidate set generation\n    candidate_set_size = max(5, len(unvisited_nodes) // 3)\n    candidate_set = sorted(unvisited_nodes, key=lambda x: distance_matrix[current_node][x])[:candidate_set_size]\n    \n    # Dynamic clustering of unvisited nodes\n    clusters = dynamic_clustering(unvisited_nodes, distance_matrix)\n    \n    # Hybrid scoring mechanism with refined score function\n    scores = []\n    for node in candidate_set:\n        remaining_after_choice = [n for n in unvisited_nodes if n != node]\n        score = refined_score(node, remaining_after_choice, clusters, distance_matrix)\n        scores.append((node, score))\n    \n    # Select the node with the lowest score\n    next_node = min(scores, key=lambda x: x[1])[0]\n    return next_node",
          "objective": 6.24817,
          "other_inf": null,
          "metadata": {
               "operator": "m2",
               "wisdom_tips": [
                    "Design adaptive hybrid meta-heuristics synergistically fusing multiple search paradigms and dynamically tune operator parameters based on search stage or problem features",
                    "Employ machine learning or pattern recognition to mine deep problem structures and optimal solution patterns then use learned insights to intelligently bias towards promising search regions or constructive choices",
                    "Explore objective function engineering by introducing auxiliary or surrogate objectives or by dynamically adjusting weights to reshape the search landscape aiding escape from local optima or guiding diverse exploration"
               ],
               "attention_focus": "refining core evaluation and scoring functions",
               "generation_tendency": "focus_exploitation",
               "timestamp": 1748448251.6810272
          }
     },
     {
          "algorithm": "node: node for node in nodes",
          "code": "import numpy as np\n\ndef select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):\n    def mst_heuristic(nodes, dist_matrix):\n        if len(nodes) == 0:\n            return 0\n        edges = [(dist_matrix[i][j], i, j) for i in nodes for j in nodes if i < j]\n        edges.sort(key=lambda x: x[0])\n        parent = {node: node for node in nodes}\n        \n        def find(u):\n            while parent[u] != u:\n                parent[u] = parent[parent[u]]\n                u = parent[u]\n            return u\n        \n        mst_cost = 0\n        for cost, u, v in edges:\n            root_u, root_v = find(u), find(v)\n            if root_u != root_v:\n                parent[root_v] = root_u\n                mst_cost += cost\n        return mst_cost\n    \n    def simplified_score(node, remaining_nodes, dist_matrix):\n        future_cost = mst_heuristic(remaining_nodes, dist_matrix)\n        return distance_matrix[current_node][node] + 0.5 * future_cost\n    \n    # Adaptive candidate set generation\n    candidate_set_size = max(3, len(unvisited_nodes) // 2)\n    candidate_set = sorted(unvisited_nodes, key=lambda x: distance_matrix[current_node][x])[:candidate_set_size]\n    \n    # Simplified scoring mechanism\n    scores = []\n    for node in candidate_set:\n        remaining_after_choice = [n for n in unvisited_nodes if n != node]\n        score = simplified_score(node, remaining_after_choice, distance_matrix)\n        scores.append((node, score))\n    \n    # Select the node with the lowest score\n    next_node = min(scores, key=lambda x: x[1])[0]\n    return next_node",
          "objective": 6.37,
          "other_inf": null,
          "metadata": {
               "operator": "m3",
               "wisdom_tips": [
                    "Design adaptive hybrid meta-heuristics synergistically fusing multiple search paradigms and dynamically tune operator parameters based on search stage or problem features",
                    "Employ machine learning or pattern recognition to mine deep problem structures and optimal solution patterns then use learned insights to intelligently bias towards promising search regions or constructive choices",
                    "Explore objective function engineering by introducing auxiliary or surrogate objectives or by dynamically adjusting weights to reshape the search landscape aiding escape from local optima or guiding diverse exploration"
               ],
               "attention_focus": "refining core evaluation and scoring functions",
               "generation_tendency": "balanced_search",
               "timestamp": 1748448287.744606
          }
     }
]