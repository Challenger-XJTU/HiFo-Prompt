[
     {
          "algorithm": "The algorithm selects the next node by dynamically weighting nearest neighbors and global path coherence using a beam search mechanism with adaptive thresholds, while simulating lightweight Monte Carlo-like rollouts to evaluate candidate nodes.}\n\n```python\nimport numpy as np\n\ndef select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):\n    def monte_carlo_rollout(node, candidates, simulations=8):\n        scores = {candidate: 0 for candidate in candidates}\n        for _ in range(simulations):\n            for candidate in candidates:\n                remaining_nodes = [n for n in unvisited_nodes if n != candidate]\n                path = [current_node, candidate]\n                total_cost = distance_matrix[current_node][candidate]\n                while remaining_nodes:\n                    next_node = min(remaining_nodes, key=lambda x: distance_matrix[path[-1]][x])\n                    path.append(next_node)\n                    total_cost += distance_matrix[path[-2]][path[-1]]\n                    remaining_nodes.remove(next_node)\n                total_cost += distance_matrix[path[-1]][destination_node]\n                scores[candidate] += total_cost\n        return {k: v / simulations for k, v in scores.items()}\n    \n    def beam_search_candidates(candidates, scores, beam_width=3):\n        sorted_candidates = sorted(candidates, key=lambda x: scores[x])\n        return sorted_candidates[:beam_width]\n    \n    # Step 1: Generate a limited candidate set based on nearest neighbors\n    all_candidates = unvisited_nodes\n    candidate_set_size = min(4, len(all_candidates))  # Fine-tuned parameter\n    candidates = sorted(all_candidates, key=lambda x: distance_matrix[current_node][x])[:candidate_set_size]\n    \n    # Step 2: Perform Monte Carlo-like rollouts to evaluate candidates\n    rollout_scores = monte_carlo_rollout(current_node, candidates)\n    \n    # Step 3: Use beam search to refine candidate selection\n    refined_candidates = beam_search_candidates(candidates, rollout_scores)\n    \n    # Step 4: Dynamically weight local edge costs with global coherence\n    weighted_scores = {\n        candidate: (distance_matrix[current_node][candidate] + rollout_scores[candidate]) / 2\n        for candidate in refined_candidates\n    ",
          "code": "import numpy as np\n\ndef select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):\n    def monte_carlo_rollout(node, candidates, simulations=8):\n        scores = {candidate: 0 for candidate in candidates}\n        for _ in range(simulations):\n            for candidate in candidates:\n                remaining_nodes = [n for n in unvisited_nodes if n != candidate]\n                path = [current_node, candidate]\n                total_cost = distance_matrix[current_node][candidate]\n                while remaining_nodes:\n                    next_node = min(remaining_nodes, key=lambda x: distance_matrix[path[-1]][x])\n                    path.append(next_node)\n                    total_cost += distance_matrix[path[-2]][path[-1]]\n                    remaining_nodes.remove(next_node)\n                total_cost += distance_matrix[path[-1]][destination_node]\n                scores[candidate] += total_cost\n        return {k: v / simulations for k, v in scores.items()}\n    \n    def beam_search_candidates(candidates, scores, beam_width=3):\n        sorted_candidates = sorted(candidates, key=lambda x: scores[x])\n        return sorted_candidates[:beam_width]\n    \n    # Step 1: Generate a limited candidate set based on nearest neighbors\n    all_candidates = unvisited_nodes\n    candidate_set_size = min(4, len(all_candidates))  # Fine-tuned parameter\n    candidates = sorted(all_candidates, key=lambda x: distance_matrix[current_node][x])[:candidate_set_size]\n    \n    # Step 2: Perform Monte Carlo-like rollouts to evaluate candidates\n    rollout_scores = monte_carlo_rollout(current_node, candidates)\n    \n    # Step 3: Use beam search to refine candidate selection\n    refined_candidates = beam_search_candidates(candidates, rollout_scores)\n    \n    # Step 4: Dynamically weight local edge costs with global coherence\n    weighted_scores = {\n        candidate: (distance_matrix[current_node][candidate] + rollout_scores[candidate]) / 2\n        for candidate in refined_candidates\n    }\n    \n    # Step 5: Select the next node with the lowest weighted score\n    next_node = min(weighted_scores, key=weighted_scores.get)\n    \n    return next_node",
          "objective": 5.98358,
          "other_inf": null,
          "metadata": {
               "operator": "m1",
               "wisdom_tips": [
                    "Balance local optimization with global solution structure when making decisions",
                    "Use dynamic weighting mechanisms to adapt algorithm behavior based on current state",
                    "Consider the impact of current choices on future decision flexibility"
               ],
               "attention_focus": "fine-tuning critical algorithm parameters and thresholds",
               "generation_tendency": "balanced_search",
               "timestamp": 1748279122.079971
          }
     },
     {
          "algorithm": "The algorithm selects the next node by dynamically weighting nearest neighbors, global path coherence, and edge cost normalization using an adaptive beam search mechanism with Monte Carlo-like rollouts, emphasizing future flexibility and local-global balance.}\n\n```python\nimport numpy as np\n\ndef select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):\n    def monte_carlo_rollout(node, candidates, simulations=15):\n        scores = {candidate: 0 for candidate in candidates}\n        for _ in range(simulations):\n            for candidate in candidates:\n                remaining_nodes = [n for n in unvisited_nodes if n != candidate]\n                path = [current_node, candidate]\n                total_cost = distance_matrix[current_node][candidate]\n                while remaining_nodes:\n                    next_node = min(remaining_nodes, key=lambda x: distance_matrix[path[-1]][x])\n                    path.append(next_node)\n                    total_cost += distance_matrix[path[-2]][path[-1]]\n                    remaining_nodes.remove(next_node)\n                total_cost += distance_matrix[path[-1]][destination_node]\n                scores[candidate] += total_cost\n        return {k: v / simulations for k, v in scores.items()}\n    \n    def beam_search_candidates(candidates, scores, beam_width=6):\n        sorted_candidates = sorted(candidates, key=lambda x: scores[x])\n        return sorted_candidates[:beam_width]\n    \n    # Step 1: Generate a limited candidate set based on nearest neighbors\n    all_candidates = unvisited_nodes\n    candidate_set_size = min(8, len(all_candidates))  # Adjusted parameter\n    candidates = sorted(all_candidates, key=lambda x: distance_matrix[current_node][x])[:candidate_set_size]\n    \n    # Step 2: Perform Monte Carlo-like rollouts to evaluate candidates\n    rollout_scores = monte_carlo_rollout(current_node, candidates)\n    \n    # Step 3: Use beam search to refine candidate selection\n    refined_candidates = beam_search_candidates(candidates, rollout_scores)\n    \n    # Step 4: Dynamically weight local edge costs with global coherence and edge cost normalization\n    weighted_scores = {\n        candidate: (0.5 * distance_matrix[current_node][candidate] + 0.5 * rollout_scores[candidate]) / \n                   (1 + 0.2 * np.std([distance_matrix[current_node][x] for x in unvisited_nodes]))\n        for candidate in refined_candidates\n    ",
          "code": "import numpy as np\n\ndef select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):\n    def monte_carlo_rollout(node, candidates, simulations=15):\n        scores = {candidate: 0 for candidate in candidates}\n        for _ in range(simulations):\n            for candidate in candidates:\n                remaining_nodes = [n for n in unvisited_nodes if n != candidate]\n                path = [current_node, candidate]\n                total_cost = distance_matrix[current_node][candidate]\n                while remaining_nodes:\n                    next_node = min(remaining_nodes, key=lambda x: distance_matrix[path[-1]][x])\n                    path.append(next_node)\n                    total_cost += distance_matrix[path[-2]][path[-1]]\n                    remaining_nodes.remove(next_node)\n                total_cost += distance_matrix[path[-1]][destination_node]\n                scores[candidate] += total_cost\n        return {k: v / simulations for k, v in scores.items()}\n    \n    def beam_search_candidates(candidates, scores, beam_width=6):\n        sorted_candidates = sorted(candidates, key=lambda x: scores[x])\n        return sorted_candidates[:beam_width]\n    \n    # Step 1: Generate a limited candidate set based on nearest neighbors\n    all_candidates = unvisited_nodes\n    candidate_set_size = min(8, len(all_candidates))  # Adjusted parameter\n    candidates = sorted(all_candidates, key=lambda x: distance_matrix[current_node][x])[:candidate_set_size]\n    \n    # Step 2: Perform Monte Carlo-like rollouts to evaluate candidates\n    rollout_scores = monte_carlo_rollout(current_node, candidates)\n    \n    # Step 3: Use beam search to refine candidate selection\n    refined_candidates = beam_search_candidates(candidates, rollout_scores)\n    \n    # Step 4: Dynamically weight local edge costs with global coherence and edge cost normalization\n    weighted_scores = {\n        candidate: (0.5 * distance_matrix[current_node][candidate] + 0.5 * rollout_scores[candidate]) / \n                   (1 + 0.2 * np.std([distance_matrix[current_node][x] for x in unvisited_nodes]))\n        for candidate in refined_candidates\n    }\n    \n    # Step 5: Select the next node with the lowest weighted score\n    next_node = min(weighted_scores, key=weighted_scores.get)\n    \n    return next_node",
          "objective": 5.98482,
          "other_inf": null,
          "metadata": {
               "operator": "m2",
               "wisdom_tips": [
                    "Balance local optimization with global solution structure when making decisions",
                    "Use dynamic weighting mechanisms to adapt algorithm behavior based on current state",
                    "Consider the impact of current choices on future decision flexibility"
               ],
               "attention_focus": "exploring novel solution construction methodologies",
               "generation_tendency": "balanced_search",
               "timestamp": 1748279346.3379269
          }
     },
     {
          "algorithm": "The new algorithm refines candidate selection by integrating adaptive look-ahead simulations, dynamic edge cost scaling, and path momentum considerations to balance local and global optimization while improving decision precision.}\n\n```python\nimport numpy as np\n\ndef select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):\n    def lookahead_simulation(node, candidates, simulations=5):\n        scores = {candidate: 0 for candidate in candidates}\n        for _ in range(simulations):\n            for candidate in candidates:\n                remaining_nodes = [n for n in unvisited_nodes if n != candidate]\n                path = [current_node, candidate]\n                total_cost = distance_matrix[current_node][candidate]\n                while remaining_nodes:\n                    next_node = min(remaining_nodes, key=lambda x: distance_matrix[path[-1]][x] + 0.1 * np.std([distance_matrix[path[-1]][y] for y in remaining_nodes]))\n                    path.append(next_node)\n                    total_cost += distance_matrix[path[-2]][path[-1]]\n                    remaining_nodes.remove(next_node)\n                total_cost += distance_matrix[path[-1]][destination_node]\n                scores[candidate] += total_cost\n        return {k: v / simulations for k, v in scores.items()}\n    \n    # Step 1: Generate a limited candidate set with adaptive size based on clustering density\n    all_candidates = unvisited_nodes\n    candidate_set_size = max(2, min(4, int(np.sqrt(len(all_candidates)))))  # Adaptive size\n    candidates = sorted(all_candidates, key=lambda x: distance_matrix[current_node][x])[:candidate_set_size]\n    \n    # Step 2: Perform refined look-ahead simulations to evaluate candidates\n    simulation_scores = lookahead_simulation(current_node, candidates)\n    \n    # Step 3: Dynamically weight edge costs with momentum and global coherence\n    weighted_scores = {\n        candidate: (0.6 * distance_matrix[current_node][candidate] + \n                    0.3 * simulation_scores[candidate] + \n                    0.1 * np.mean([distance_matrix[current_node][x] for x in unvisited_nodes])) / \n                   (1 + 0.05 * np.std([distance_matrix[current_node][x] for x in unvisited_nodes]))\n        for candidate in candidates\n    ",
          "code": "import numpy as np\n\ndef select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):\n    def lookahead_simulation(node, candidates, simulations=5):\n        scores = {candidate: 0 for candidate in candidates}\n        for _ in range(simulations):\n            for candidate in candidates:\n                remaining_nodes = [n for n in unvisited_nodes if n != candidate]\n                path = [current_node, candidate]\n                total_cost = distance_matrix[current_node][candidate]\n                while remaining_nodes:\n                    next_node = min(remaining_nodes, key=lambda x: distance_matrix[path[-1]][x] + 0.1 * np.std([distance_matrix[path[-1]][y] for y in remaining_nodes]))\n                    path.append(next_node)\n                    total_cost += distance_matrix[path[-2]][path[-1]]\n                    remaining_nodes.remove(next_node)\n                total_cost += distance_matrix[path[-1]][destination_node]\n                scores[candidate] += total_cost\n        return {k: v / simulations for k, v in scores.items()}\n    \n    # Step 1: Generate a limited candidate set with adaptive size based on clustering density\n    all_candidates = unvisited_nodes\n    candidate_set_size = max(2, min(4, int(np.sqrt(len(all_candidates)))))  # Adaptive size\n    candidates = sorted(all_candidates, key=lambda x: distance_matrix[current_node][x])[:candidate_set_size]\n    \n    # Step 2: Perform refined look-ahead simulations to evaluate candidates\n    simulation_scores = lookahead_simulation(current_node, candidates)\n    \n    # Step 3: Dynamically weight edge costs with momentum and global coherence\n    weighted_scores = {\n        candidate: (0.6 * distance_matrix[current_node][candidate] + \n                    0.3 * simulation_scores[candidate] + \n                    0.1 * np.mean([distance_matrix[current_node][x] for x in unvisited_nodes])) / \n                   (1 + 0.05 * np.std([distance_matrix[current_node][x] for x in unvisited_nodes]))\n        for candidate in candidates\n    }\n    \n    # Step 4: Select the next node with the lowest weighted score\n    next_node = min(weighted_scores, key=weighted_scores.get)\n    \n    return next_node",
          "objective": 5.99083,
          "other_inf": null,
          "metadata": {
               "operator": "m1",
               "wisdom_tips": [
                    "Balance local optimization with global solution structure when making decisions",
                    "Use dynamic weighting mechanisms to adapt algorithm behavior based on current state",
                    "Consider the impact of current choices on future decision flexibility"
               ],
               "attention_focus": "improving precision of existing heuristics and rules",
               "generation_tendency": "focus_exploitation",
               "timestamp": 1748279275.368505
          }
     },
     {
          "algorithm": "The algorithm selects the next node by dynamically weighting nearest neighbors, global path coherence, and edge cost normalization using an adaptive beam search mechanism with Monte Carlo-like rollouts, emphasizing future flexibility and local-global balance.}\n\n```python\nimport numpy as np\n\ndef select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):\n    def monte_carlo_rollout(node, candidates, simulations=10):\n        scores = {candidate: 0 for candidate in candidates}\n        for _ in range(simulations):\n            for candidate in candidates:\n                remaining_nodes = [n for n in unvisited_nodes if n != candidate]\n                path = [current_node, candidate]\n                total_cost = distance_matrix[current_node][candidate]\n                while remaining_nodes:\n                    next_node = min(remaining_nodes, key=lambda x: distance_matrix[path[-1]][x])\n                    path.append(next_node)\n                    total_cost += distance_matrix[path[-2]][path[-1]]\n                    remaining_nodes.remove(next_node)\n                total_cost += distance_matrix[path[-1]][destination_node]\n                scores[candidate] += total_cost\n        return {k: v / simulations for k, v in scores.items()}\n    \n    def beam_search_candidates(candidates, scores, beam_width=4):\n        sorted_candidates = sorted(candidates, key=lambda x: scores[x])\n        return sorted_candidates[:beam_width]\n    \n    # Step 1: Generate a limited candidate set based on nearest neighbors\n    all_candidates = unvisited_nodes\n    candidate_set_size = min(5, len(all_candidates))  # Adjusted parameter\n    candidates = sorted(all_candidates, key=lambda x: distance_matrix[current_node][x])[:candidate_set_size]\n    \n    # Step 2: Perform Monte Carlo-like rollouts to evaluate candidates\n    rollout_scores = monte_carlo_rollout(current_node, candidates)\n    \n    # Step 3: Use beam search to refine candidate selection\n    refined_candidates = beam_search_candidates(candidates, rollout_scores)\n    \n    # Step 4: Dynamically weight local edge costs with global coherence and edge cost normalization\n    weighted_scores = {\n        candidate: (0.6 * distance_matrix[current_node][candidate] + 0.4 * rollout_scores[candidate]) / \n                   (1 + 0.1 * np.std([distance_matrix[current_node][x] for x in unvisited_nodes]))\n        for candidate in refined_candidates\n    ",
          "code": "import numpy as np\n\ndef select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):\n    def monte_carlo_rollout(node, candidates, simulations=10):\n        scores = {candidate: 0 for candidate in candidates}\n        for _ in range(simulations):\n            for candidate in candidates:\n                remaining_nodes = [n for n in unvisited_nodes if n != candidate]\n                path = [current_node, candidate]\n                total_cost = distance_matrix[current_node][candidate]\n                while remaining_nodes:\n                    next_node = min(remaining_nodes, key=lambda x: distance_matrix[path[-1]][x])\n                    path.append(next_node)\n                    total_cost += distance_matrix[path[-2]][path[-1]]\n                    remaining_nodes.remove(next_node)\n                total_cost += distance_matrix[path[-1]][destination_node]\n                scores[candidate] += total_cost\n        return {k: v / simulations for k, v in scores.items()}\n    \n    def beam_search_candidates(candidates, scores, beam_width=4):\n        sorted_candidates = sorted(candidates, key=lambda x: scores[x])\n        return sorted_candidates[:beam_width]\n    \n    # Step 1: Generate a limited candidate set based on nearest neighbors\n    all_candidates = unvisited_nodes\n    candidate_set_size = min(5, len(all_candidates))  # Adjusted parameter\n    candidates = sorted(all_candidates, key=lambda x: distance_matrix[current_node][x])[:candidate_set_size]\n    \n    # Step 2: Perform Monte Carlo-like rollouts to evaluate candidates\n    rollout_scores = monte_carlo_rollout(current_node, candidates)\n    \n    # Step 3: Use beam search to refine candidate selection\n    refined_candidates = beam_search_candidates(candidates, rollout_scores)\n    \n    # Step 4: Dynamically weight local edge costs with global coherence and edge cost normalization\n    weighted_scores = {\n        candidate: (0.6 * distance_matrix[current_node][candidate] + 0.4 * rollout_scores[candidate]) / \n                   (1 + 0.1 * np.std([distance_matrix[current_node][x] for x in unvisited_nodes]))\n        for candidate in refined_candidates\n    }\n    \n    # Step 5: Select the next node with the lowest weighted score\n    next_node = min(weighted_scores, key=weighted_scores.get)\n    \n    return next_node",
          "objective": 5.99414,
          "other_inf": null,
          "metadata": {
               "operator": "m2",
               "wisdom_tips": [
                    "Balance local optimization with global solution structure when making decisions",
                    "Use dynamic weighting mechanisms to adapt algorithm behavior based on current state",
                    "Consider the impact of current choices on future decision flexibility"
               ],
               "attention_focus": "refining core evaluation and scoring functions",
               "generation_tendency": "focus_exploitation",
               "timestamp": 1748279155.5841851
          }
     }
]