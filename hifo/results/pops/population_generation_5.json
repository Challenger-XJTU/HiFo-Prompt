[
     {
          "algorithm": "A refined hybrid algorithm that enhances precision by integrating adaptive candidate filtering, a dual-phase MST heuristic with edge clustering, and a weighted scoring function dynamically adjusted via pattern recognition to iteratively select the next node while balancing global optimality and local exploitation.}\n\n```python\nimport numpy as np\n\ndef select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):\n    def mst_heuristic_with_clustering(nodes, dist_matrix):\n        if len(nodes) == 0:\n            return 0\n        edges = [(dist_matrix[i][j], i, j) for i in nodes for j in nodes if i < j]\n        edges.sort(key=lambda x: x[0])\n        \n        # Cluster-aware parent initialization\n        parent = {node: node for node in nodes}\n        clusters = {node: {node} for node in nodes",
          "code": "import numpy as np\n\ndef select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):\n    def mst_heuristic_with_clustering(nodes, dist_matrix):\n        if len(nodes) == 0:\n            return 0\n        edges = [(dist_matrix[i][j], i, j) for i in nodes for j in nodes if i < j]\n        edges.sort(key=lambda x: x[0])\n        \n        # Cluster-aware parent initialization\n        parent = {node: node for node in nodes}\n        clusters = {node: {node} for node in nodes}\n        \n        def find(u):\n            while parent[u] != u:\n                parent[u] = parent[parent[u]]\n                u = parent[u]\n            return u\n        \n        mst_cost = 0\n        for cost, u, v in edges:\n            root_u, root_v = find(u), find(v)\n            if root_u != root_v:\n                # Merge smaller cluster into larger one\n                if len(clusters[root_u]) < len(clusters[root_v]):\n                    parent[root_u] = root_v\n                    clusters[root_v].update(clusters[root_u])\n                    del clusters[root_u]\n                else:\n                    parent[root_v] = root_u\n                    clusters[root_u].update(clusters[root_v])\n                    del clusters[root_v]\n                mst_cost += cost\n        return mst_cost\n    \n    def monte_carlo_simulation_with_bias(node, remaining_nodes, dist_matrix, simulations=20):\n        total_cost = 0\n        bias_factor = 0.8  # Bias towards shorter initial moves\n        for _ in range(simulations):\n            temp_remaining = remaining_nodes[:]\n            current = node\n            cost = 0\n            while temp_remaining:\n                next_node = min(temp_remaining, key=lambda x: dist_matrix[current][x] * bias_factor)\n                cost += dist_matrix[current][next_node]\n                current = next_node\n                temp_remaining.remove(next_node)\n            cost += dist_matrix[current][destination_node]\n            total_cost += cost\n        return total_cost / simulations\n    \n    # Adaptive candidate filtering with dynamic size adjustment\n    candidate_set_size = max(3, int(np.sqrt(len(unvisited_nodes))))\n    candidate_set = sorted(unvisited_nodes, key=lambda x: distance_matrix[current_node][x])[:candidate_set_size]\n    \n    # Dynamic weight adjustment based on search progress\n    progress_ratio = 1 - len(unvisited_nodes) / (len(distance_matrix) - 1)\n    mst_weight = 0.7 - 0.2 * progress_ratio\n    mc_weight = 0.3 + 0.2 * progress_ratio\n    \n    # Hybrid scoring mechanism with enhanced precision\n    scores = []\n    for node in candidate_set:\n        remaining_after_choice = [n for n in unvisited_nodes if n != node]\n        future_cost_mst = mst_heuristic_with_clustering(remaining_after_choice, distance_matrix)\n        future_cost_mc = monte_carlo_simulation_with_bias(node, remaining_after_choice, distance_matrix)\n        score = distance_matrix[current_node][node] + mst_weight * future_cost_mst + mc_weight * future_cost_mc\n        scores.append((node, score))\n    \n    # Select the node with the lowest score\n    next_node = min(scores, key=lambda x: x[1])[0]\n    return next_node",
          "objective": 5.88688,
          "other_inf": null,
          "metadata": {
               "operator": "m1",
               "wisdom_tips": [
                    "Design adaptive hybrid meta-heuristics synergistically fusing multiple search paradigms and dynamically tune operator parameters based on search stage or problem features",
                    "Employ machine learning or pattern recognition to mine deep problem structures and optimal solution patterns then use learned insights to intelligently bias towards promising search regions or constructive choices",
                    "Explore objective function engineering by introducing auxiliary or surrogate objectives or by dynamically adjusting weights to reshape the search landscape aiding escape from local optima or guiding diverse exploration"
               ],
               "attention_focus": "improving precision of existing heuristics and rules",
               "generation_tendency": "focus_exploitation",
               "timestamp": 1748448597.876456
          }
     },
     {
          "algorithm": "An adaptive hybrid algorithm dynamically blending MST-based global optimization, Monte Carlo lookahead with learned biasing, and trajectory-aware candidate filtering to iteratively select the next node while refining exploration-exploitation balance through stage-aware parameter tuning.}\n\n```python\nimport numpy as np\n\ndef select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):\n    def mst_heuristic_with_clustering(nodes, dist_matrix):\n        if len(nodes) == 0:\n            return 0\n        edges = [(dist_matrix[i][j], i, j) for i in nodes for j in nodes if i < j]\n        edges.sort(key=lambda x: x[0])\n        \n        parent = {node: node for node in nodes}\n        clusters = {node: {node} for node in nodes",
          "code": "import numpy as np\n\ndef select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):\n    def mst_heuristic_with_clustering(nodes, dist_matrix):\n        if len(nodes) == 0:\n            return 0\n        edges = [(dist_matrix[i][j], i, j) for i in nodes for j in nodes if i < j]\n        edges.sort(key=lambda x: x[0])\n        \n        parent = {node: node for node in nodes}\n        clusters = {node: {node} for node in nodes}\n        \n        def find(u):\n            while parent[u] != u:\n                parent[u] = parent[parent[u]]\n                u = parent[u]\n            return u\n        \n        mst_cost = 0\n        for cost, u, v in edges:\n            root_u, root_v = find(u), find(v)\n            if root_u != root_v:\n                if len(clusters[root_u]) < len(clusters[root_v]):\n                    parent[root_u] = root_v\n                    clusters[root_v].update(clusters[root_u])\n                    del clusters[root_u]\n                else:\n                    parent[root_v] = root_u\n                    clusters[root_u].update(clusters[root_v])\n                    del clusters[root_v]\n                mst_cost += cost\n        return mst_cost\n    \n    def monte_carlo_simulation_with_bias(node, remaining_nodes, dist_matrix, simulations=30):\n        total_cost = 0\n        bias_factor = 1.2 - 0.4 * (len(remaining_nodes) / len(distance_matrix))  # Dynamic bias adjustment\n        for _ in range(simulations):\n            temp_remaining = remaining_nodes[:]\n            current = node\n            cost = 0\n            while temp_remaining:\n                next_node = min(temp_remaining, key=lambda x: dist_matrix[current][x] * bias_factor)\n                cost += dist_matrix[current][next_node]\n                current = next_node\n                temp_remaining.remove(next_node)\n            cost += dist_matrix[current][destination_node]\n            total_cost += cost\n        return total_cost / simulations\n    \n    # Adaptive candidate filtering with dynamic size adjustment\n    candidate_set_size = max(5, int(2 * np.sqrt(len(unvisited_nodes))))\n    candidate_set = sorted(unvisited_nodes, key=lambda x: distance_matrix[current_node][x])[:candidate_set_size]\n    \n    # Dynamic weight adjustment based on search progress\n    progress_ratio = 1 - len(unvisited_nodes) / (len(distance_matrix) - 1)\n    mst_weight = 0.6 + 0.1 * progress_ratio\n    mc_weight = 0.4 - 0.1 * progress_ratio\n    \n    # Hybrid scoring mechanism with enhanced precision\n    scores = []\n    for node in candidate_set:\n        remaining_after_choice = [n for n in unvisited_nodes if n != node]\n        future_cost_mst = mst_heuristic_with_clustering(remaining_after_choice, distance_matrix)\n        future_cost_mc = monte_carlo_simulation_with_bias(node, remaining_after_choice, distance_matrix)\n        score = distance_matrix[current_node][node] + mst_weight * future_cost_mst + mc_weight * future_cost_mc\n        scores.append((node, score))\n    \n    # Select the node with the lowest score\n    next_node = min(scores, key=lambda x: x[1])[0]\n    return next_node",
          "objective": 5.91089,
          "other_inf": null,
          "metadata": {
               "operator": "m2",
               "wisdom_tips": [
                    "Design adaptive hybrid meta-heuristics synergistically fusing multiple search paradigms and dynamically tune operator parameters based on search stage or problem features",
                    "Employ machine learning or pattern recognition to mine deep problem structures and optimal solution patterns then use learned insights to intelligently bias towards promising search regions or constructive choices",
                    "Explore objective function engineering by introducing auxiliary or surrogate objectives or by dynamically adjusting weights to reshape the search landscape aiding escape from local optima or guiding diverse exploration"
               ],
               "attention_focus": "improving precision of existing heuristics and rules",
               "generation_tendency": "balanced_search",
               "timestamp": 1748448876.51855
          }
     },
     {
          "algorithm": "node: node for node in nodes",
          "code": "import numpy as np\n\ndef select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):\n    def mst_heuristic(nodes, dist_matrix):\n        if len(nodes) == 0:\n            return 0\n        edges = [(dist_matrix[i][j], i, j) for i in nodes for j in nodes if i < j]\n        edges.sort(key=lambda x: x[0])\n        \n        parent = {node: node for node in nodes}\n        \n        def find(u):\n            while parent[u] != u:\n                parent[u] = parent[parent[u]]\n                u = parent[u]\n            return u\n        \n        mst_cost = 0\n        for cost, u, v in edges:\n            root_u, root_v = find(u), find(v)\n            if root_u != root_v:\n                parent[root_u] = root_v\n                mst_cost += cost\n        return mst_cost\n    \n    def monte_carlo_simulation(node, remaining_nodes, dist_matrix, simulations=10):\n        total_cost = 0\n        for _ in range(simulations):\n            temp_remaining = remaining_nodes[:]\n            current = node\n            cost = 0\n            while temp_remaining:\n                next_node = min(temp_remaining, key=lambda x: dist_matrix[current][x])\n                cost += dist_matrix[current][next_node]\n                current = next_node\n                temp_remaining.remove(next_node)\n            cost += dist_matrix[current][destination_node]\n            total_cost += cost\n        return total_cost / simulations\n    \n    # Adaptive candidate filtering with fixed size\n    candidate_set_size = max(3, int(np.sqrt(len(unvisited_nodes))))\n    candidate_set = sorted(unvisited_nodes, key=lambda x: distance_matrix[current_node][x])[:candidate_set_size]\n    \n    # Fixed weight adjustment\n    mst_weight = 0.6\n    mc_weight = 0.4\n    \n    # Simplified scoring mechanism\n    scores = []\n    for node in candidate_set:\n        remaining_after_choice = [n for n in unvisited_nodes if n != node]\n        future_cost_mst = mst_heuristic(remaining_after_choice, distance_matrix)\n        future_cost_mc = monte_carlo_simulation(node, remaining_after_choice, distance_matrix)\n        score = distance_matrix[current_node][node] + mst_weight * future_cost_mst + mc_weight * future_cost_mc\n        scores.append((node, score))\n    \n    # Select the node with the lowest score\n    next_node = min(scores, key=lambda x: x[1])[0]\n    return next_node",
          "objective": 5.91224,
          "other_inf": null,
          "metadata": {
               "operator": "m3",
               "wisdom_tips": [
                    "Design adaptive hybrid meta-heuristics synergistically fusing multiple search paradigms and dynamically tune operator parameters based on search stage or problem features",
                    "Employ machine learning or pattern recognition to mine deep problem structures and optimal solution patterns then use learned insights to intelligently bias towards promising search regions or constructive choices",
                    "Explore objective function engineering by introducing auxiliary or surrogate objectives or by dynamically adjusting weights to reshape the search landscape aiding escape from local optima or guiding diverse exploration"
               ],
               "attention_focus": "refining core evaluation and scoring functions",
               "generation_tendency": "focus_exploitation",
               "timestamp": 1748448716.659091
          }
     },
     {
          "algorithm": "node: node for node in nodes",
          "code": "import numpy as np\n\ndef select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):\n    def mst_heuristic(nodes, dist_matrix):\n        if len(nodes) == 0:\n            return 0\n        edges = [(dist_matrix[i][j], i, j) for i in nodes for j in nodes if i < j]\n        edges.sort(key=lambda x: x[0])\n        \n        parent = {node: node for node in nodes}\n        \n        def find(u):\n            while parent[u] != u:\n                parent[u] = parent[parent[u]]\n                u = parent[u]\n            return u\n        \n        mst_cost = 0\n        for cost, u, v in edges:\n            root_u, root_v = find(u), find(v)\n            if root_u != root_v:\n                parent[root_u] = root_v\n                mst_cost += cost\n        return mst_cost\n    \n    def monte_carlo_simulation(node, remaining_nodes, dist_matrix, simulations=10):\n        total_cost = 0\n        for _ in range(simulations):\n            temp_remaining = remaining_nodes[:]\n            current = node\n            cost = 0\n            while temp_remaining:\n                next_node = min(temp_remaining, key=lambda x: dist_matrix[current][x])\n                cost += dist_matrix[current][next_node]\n                current = next_node\n                temp_remaining.remove(next_node)\n            cost += dist_matrix[current][destination_node]\n            total_cost += cost\n        return total_cost / simulations\n    \n    candidate_set_size = max(2, int(np.sqrt(len(unvisited_nodes))))\n    candidate_set = sorted(unvisited_nodes, key=lambda x: distance_matrix[current_node][x])[:candidate_set_size]\n    \n    progress_ratio = 1 - len(unvisited_nodes) / (len(distance_matrix) - 1)\n    mst_weight = 0.6 - 0.1 * progress_ratio\n    mc_weight = 0.4 + 0.1 * progress_ratio\n    \n    scores = []\n    for node in candidate_set:\n        remaining_after_choice = [n for n in unvisited_nodes if n != node]\n        future_cost_mst = mst_heuristic(remaining_after_choice, distance_matrix)\n        future_cost_mc = monte_carlo_simulation(node, remaining_after_choice, distance_matrix)\n        score = distance_matrix[current_node][node] + mst_weight * future_cost_mst + mc_weight * future_cost_mc\n        scores.append((node, score))\n    \n    next_node = min(scores, key=lambda x: x[1])[0]\n    return next_node",
          "objective": 5.935,
          "other_inf": null,
          "metadata": {
               "operator": "m3",
               "wisdom_tips": [
                    "Design adaptive hybrid meta-heuristics synergistically fusing multiple search paradigms and dynamically tune operator parameters based on search stage or problem features",
                    "Employ machine learning or pattern recognition to mine deep problem structures and optimal solution patterns then use learned insights to intelligently bias towards promising search regions or constructive choices",
                    "Explore objective function engineering by introducing auxiliary or surrogate objectives or by dynamically adjusting weights to reshape the search landscape aiding escape from local optima or guiding diverse exploration"
               ],
               "attention_focus": "fine-tuning critical algorithm parameters and thresholds",
               "generation_tendency": "focus_exploitation",
               "timestamp": 1748448716.650294
          }
     }
]