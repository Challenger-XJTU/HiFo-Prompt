[
     {
          "algorithm": "The algorithm selects the next node by iteratively evaluating candidate sets using look-ahead simulations, MST-inspired edge cost normalization, path momentum, and clustering insights, while leveraging lightweight Monte Carlo Tree Search reasoning and beam search-guided exploration to balance short-term gains with long-term tour quality.}\n\n```python\nimport numpy as np\n\ndef select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):\n    def mst_cost(nodes):\n        if len(nodes) <= 1:\n            return 0\n        edges = sorted([(distance_matrix[n1][n2], n1, n2) for i, n1 in enumerate(nodes) for n2 in nodes[i+1:]])\n        parent = {node: node for node in nodes}\n        rank = {node: 0 for node in nodes",
          "code": "import numpy as np\n\ndef select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):\n    def mst_cost(nodes):\n        if len(nodes) <= 1:\n            return 0\n        edges = sorted([(distance_matrix[n1][n2], n1, n2) for i, n1 in enumerate(nodes) for n2 in nodes[i+1:]])\n        parent = {node: node for node in nodes}\n        rank = {node: 0 for node in nodes}\n        \n        def find(node):\n            while parent[node] != node:\n                parent[node] = parent[parent[node]]\n                node = parent[node]\n            return node\n        \n        def union(n1, n2):\n            root1, root2 = find(n1), find(n2)\n            if root1 == root2:\n                return False\n            if rank[root1] > rank[root2]:\n                parent[root2] = root1\n            else:\n                parent[root1] = root2\n                if rank[root1] == rank[root2]:\n                    rank[root2] += 1\n            return True\n        \n        total_cost = 0\n        for cost, n1, n2 in edges:\n            if union(n1, n2):\n                total_cost += cost\n        return total_cost\n    \n    def simulate_path(start, remaining, steps=3):\n        if steps == 0 or len(remaining) == 0:\n            return 0\n        best_simulated_cost = float('inf')\n        for next_n in remaining:\n            new_remaining = [n for n in remaining if n != next_n]\n            simulated_cost = distance_matrix[start][next_n] + simulate_path(next_n, new_remaining, steps-1)\n            best_simulated_cost = min(best_simulated_cost, simulated_cost)\n        return best_simulated_cost\n    \n    candidates = unvisited_nodes[:min(5, len(unvisited_nodes))]  # Limit candidate set size\n    scores = []\n    \n    for candidate in candidates:\n        future_nodes = [n for n in unvisited_nodes if n != candidate]\n        look_ahead_cost = simulate_path(candidate, future_nodes)\n        mst_remaining_cost = mst_cost(future_nodes + [destination_node])\n        normalized_edge_cost = distance_matrix[current_node][candidate] / (np.mean(distance_matrix[current_node]) + 1e-6)\n        momentum_direction = np.dot(np.array([distance_matrix[current_node][candidate]]),\n                                    np.array([distance_matrix[candidate][destination_node]]))\n        score = look_ahead_cost + mst_remaining_cost + normalized_edge_cost - momentum_direction\n        scores.append(score)\n    \n    next_node = candidates[np.argmin(scores)]\n    return next_node",
          "objective": 6.44188,
          "other_inf": null,
          "metadata": {
               "operator": "i1",
               "wisdom_tips": [
                    "Design adaptive hybrid meta-heuristics synergistically fusing multiple search paradigms and dynamically tune operator parameters based on search stage or problem features",
                    "Employ machine learning or pattern recognition to mine deep problem structures and optimal solution patterns then use learned insights to intelligently bias towards promising search regions or constructive choices",
                    "Explore objective function engineering by introducing auxiliary or surrogate objectives or by dynamically adjusting weights to reshape the search landscape aiding escape from local optima or guiding diverse exploration"
               ],
               "attention_focus": "considering long-term impact of current decisions",
               "generation_tendency": "balanced_search",
               "timestamp": 1748447722.06198
          }
     },
     {
          "algorithm": "The algorithm iteratively selects the next node by evaluating a limited set of promising neighbors through a hybrid strategy combining look-ahead evaluation, MST-based cost normalization, path momentum, clustering insights, MCTS-inspired statistical reasoning, and beam search-guided exploration to balance local refinement and global optimization.}\n\n```python\nimport numpy as np\n\ndef select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):\n    def mst_heuristic(nodes, dist_matrix):\n        if len(nodes) == 0:\n            return 0\n        edges = [(dist_matrix[i][j], i, j) for i in nodes for j in nodes if i != j]\n        edges.sort(key=lambda x: x[0])\n        parent = {node: node for node in nodes}\n        rank = {node: 0 for node in nodes",
          "code": "import numpy as np\n\ndef select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):\n    def mst_heuristic(nodes, dist_matrix):\n        if len(nodes) == 0:\n            return 0\n        edges = [(dist_matrix[i][j], i, j) for i in nodes for j in nodes if i != j]\n        edges.sort(key=lambda x: x[0])\n        parent = {node: node for node in nodes}\n        rank = {node: 0 for node in nodes}\n        \n        def find(u):\n            while parent[u] != u:\n                parent[u] = parent[parent[u]]\n                u = parent[u]\n            return u\n        \n        def union(u, v):\n            root_u = find(u)\n            root_v = find(v)\n            if root_u != root_v:\n                if rank[root_u] > rank[root_v]:\n                    parent[root_v] = root_u\n                elif rank[root_u] < rank[root_v]:\n                    parent[root_u] = root_v\n                else:\n                    parent[root_v] = root_u\n                    rank[root_u] += 1\n        \n        mst_cost = 0\n        for cost, u, v in edges:\n            if find(u) != find(v):\n                union(u, v)\n                mst_cost += cost\n        return mst_cost\n    \n    def look_ahead_score(node, remaining_nodes, dist_matrix):\n        if len(remaining_nodes) == 0:\n            return dist_matrix[node][destination_node]\n        return dist_matrix[current_node][node] + mst_heuristic(remaining_nodes, dist_matrix)\n    \n    def cluster_affinity(node, clusters, dist_matrix):\n        return min([np.mean([dist_matrix[node][n] for n in cluster]) for cluster in clusters])\n    \n    # Precompute clusters using simple k-means-like grouping based on distances\n    from sklearn.cluster import KMeans\n    coords = np.array([[i, np.mean(distance_matrix[i])] for i in unvisited_nodes])\n    kmeans = KMeans(n_clusters=min(3, len(unvisited_nodes)), random_state=0).fit(coords)\n    clusters = [list(np.array(unvisited_nodes)[kmeans.labels_ == i]) for i in range(kmeans.n_clusters)]\n    \n    # Candidate set generation\n    candidate_set = sorted(unvisited_nodes, key=lambda x: distance_matrix[current_node][x])[:min(5, len(unvisited_nodes))]\n    \n    # Beam search setup\n    beam_width = 3\n    partial_paths = [(current_node, unvisited_nodes, 0)]  # (last_node, remaining_nodes, cumulative_cost)\n    \n    for _ in range(beam_width):\n        new_paths = []\n        for last_node, remaining, cum_cost in partial_paths:\n            for neighbor in candidate_set:\n                if neighbor in remaining:\n                    new_remaining = [n for n in remaining if n != neighbor]\n                    new_cum_cost = cum_cost + distance_matrix[last_node][neighbor]\n                    new_paths.append((neighbor, new_remaining, new_cum_cost))\n        # Select top beam_width paths based on cumulative cost + MST heuristic\n        new_paths.sort(key=lambda x: x[2] + mst_heuristic(x[1], distance_matrix))\n        partial_paths = new_paths[:beam_width]\n    \n    # Monte Carlo Tree Search-inspired scoring\n    scores = []\n    for node in candidate_set:\n        remaining_after_choice = [n for n in unvisited_nodes if n != node]\n        score = (\n            distance_matrix[current_node][node] +  # Immediate cost\n            look_ahead_score(node, remaining_after_choice, distance_matrix) +  # Look-ahead evaluation\n            cluster_affinity(node, clusters, distance_matrix)  # Cluster-based affinity\n        )\n        scores.append((node, score))\n    \n    # Select the node with the lowest score\n    next_node = min(scores, key=lambda x: x[1])[0]\n    return next_node",
          "objective": 6.76746,
          "other_inf": null,
          "metadata": {
               "operator": "i1",
               "wisdom_tips": [
                    "Design adaptive hybrid meta-heuristics synergistically fusing multiple search paradigms and dynamically tune operator parameters based on search stage or problem features",
                    "Employ machine learning or pattern recognition to mine deep problem structures and optimal solution patterns then use learned insights to intelligently bias towards promising search regions or constructive choices",
                    "Explore objective function engineering by introducing auxiliary or surrogate objectives or by dynamically adjusting weights to reshape the search landscape aiding escape from local optima or guiding diverse exploration"
               ],
               "attention_focus": "managing computational complexity and time efficiency",
               "generation_tendency": "balanced_search",
               "timestamp": 1748447722.0576398
          }
     },
     {
          "algorithm": "The algorithm iteratively selects the next node by maintaining a candidate set of promising neighbors, evaluating look-ahead paths via lightweight Monte Carlo simulations, incorporating global MST properties and edge cost normalization, leveraging spatial clustering and path momentum, and using beam search to manage diverse high-quality partial tours while dynamically balancing exploration and exploitation.}\n\n```python\nimport numpy as np\n\ndef select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):\n    def calculate_mst_cost(nodes, dist_matrix):\n        if len(nodes) == 0:\n            return 0\n        edges = [(dist_matrix[i][j], i, j) for i in nodes for j in nodes if i < j]\n        edges.sort(key=lambda x: x[0])\n        parent = {node: node for node in nodes}\n        rank = {node: 0 for node in nodes}\n\n        def find(u):\n            while parent[u] != u:\n                parent[u] = parent[parent[u]]\n                u = parent[u]\n            return u\n\n        def union(u, v):\n            root_u = find(u)\n            root_v = find(v)\n            if root_u != root_v:\n                if rank[root_u] > rank[root_v]:\n                    parent[root_v] = root_u\n                elif rank[root_u] < rank[root_v]:\n                    parent[root_u] = root_v\n                else:\n                    parent[root_v] = root_u\n                    rank[root_u] += 1\n\n        mst_cost = 0\n        for cost, u, v in edges:\n            if find(u) != find(v):\n                union(u, v)\n                mst_cost += cost\n        return mst_cost\n\n    def simulate_path(node, remaining_nodes, depth=3):\n        if depth == 0 or len(remaining_nodes) == 0:\n            return 0\n        candidates = sorted(remaining_nodes, key=lambda x: distance_matrix[node][x])[:5]\n        return min(distance_matrix[node][cand] + simulate_path(cand, [n for n in remaining_nodes if n != cand], depth - 1) for cand in candidates)\n\n    def cluster_score(node, cluster_centers):\n        return min(distance_matrix[node][center] for center in cluster_centers)\n\n    # Candidate set: top 5 nearest neighbors\n    candidates = sorted(unvisited_nodes, key=lambda x: distance_matrix[current_node][x])[:5]\n\n    # Look-ahead evaluation via lightweight Monte Carlo simulation\n    lookahead_scores = {cand: simulate_path(cand, [n for n in unvisited_nodes if n != cand]) for cand in candidates}\n\n    # Minimum spanning tree property consideration\n    mst_cost_with_current = calculate_mst_cost([current_node] + list(unvisited_nodes), distance_matrix)\n    mst_costs = {cand: calculate_mst_cost([cand] + [n for n in unvisited_nodes if n != cand], distance_matrix) for cand in candidates}\n\n    # Edge cost normalization (relative to average distance from current node)\n    avg_distance = np.mean([distance_matrix[current_node][n] for n in unvisited_nodes])\n    normalized_costs = {cand: distance_matrix[current_node][cand] / avg_distance for cand in candidates}\n\n    # Path momentum and direction (favor nodes in similar direction to previous movement)\n    momentum_scores = {cand: np.dot(np.array([distance_matrix[current_node][cand], distance_matrix[cand][destination_node]]),\n                                    np.array([1, -1])) for cand in candidates}\n\n    # Nearest neighbor clusters (identify clusters and favor nodes close to cluster centers)\n    cluster_centers = [unvisited_nodes[i] for i in np.random.choice(len(unvisited_nodes), min(3, len(unvisited_nodes)), replace=False)]\n    cluster_scores = {cand: cluster_score(cand, cluster_centers) for cand in candidates}\n\n    # Combine scores into a weighted evaluation function\n    combined_scores = {\n        cand: (0.3 * lookahead_scores[cand]) +\n              (0.2 * mst_costs[cand] / mst_cost_with_current) +\n              (0.1 * normalized_costs[cand]) +\n              (0.2 * momentum_scores[cand]) +\n              (0.2 * cluster_scores[cand])\n        for cand in candidates\n    ",
          "code": "import numpy as np\n\ndef select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):\n    def calculate_mst_cost(nodes, dist_matrix):\n        if len(nodes) == 0:\n            return 0\n        edges = [(dist_matrix[i][j], i, j) for i in nodes for j in nodes if i < j]\n        edges.sort(key=lambda x: x[0])\n        parent = {node: node for node in nodes}\n        rank = {node: 0 for node in nodes}\n\n        def find(u):\n            while parent[u] != u:\n                parent[u] = parent[parent[u]]\n                u = parent[u]\n            return u\n\n        def union(u, v):\n            root_u = find(u)\n            root_v = find(v)\n            if root_u != root_v:\n                if rank[root_u] > rank[root_v]:\n                    parent[root_v] = root_u\n                elif rank[root_u] < rank[root_v]:\n                    parent[root_u] = root_v\n                else:\n                    parent[root_v] = root_u\n                    rank[root_u] += 1\n\n        mst_cost = 0\n        for cost, u, v in edges:\n            if find(u) != find(v):\n                union(u, v)\n                mst_cost += cost\n        return mst_cost\n\n    def simulate_path(node, remaining_nodes, depth=3):\n        if depth == 0 or len(remaining_nodes) == 0:\n            return 0\n        candidates = sorted(remaining_nodes, key=lambda x: distance_matrix[node][x])[:5]\n        return min(distance_matrix[node][cand] + simulate_path(cand, [n for n in remaining_nodes if n != cand], depth - 1) for cand in candidates)\n\n    def cluster_score(node, cluster_centers):\n        return min(distance_matrix[node][center] for center in cluster_centers)\n\n    # Candidate set: top 5 nearest neighbors\n    candidates = sorted(unvisited_nodes, key=lambda x: distance_matrix[current_node][x])[:5]\n\n    # Look-ahead evaluation via lightweight Monte Carlo simulation\n    lookahead_scores = {cand: simulate_path(cand, [n for n in unvisited_nodes if n != cand]) for cand in candidates}\n\n    # Minimum spanning tree property consideration\n    mst_cost_with_current = calculate_mst_cost([current_node] + list(unvisited_nodes), distance_matrix)\n    mst_costs = {cand: calculate_mst_cost([cand] + [n for n in unvisited_nodes if n != cand], distance_matrix) for cand in candidates}\n\n    # Edge cost normalization (relative to average distance from current node)\n    avg_distance = np.mean([distance_matrix[current_node][n] for n in unvisited_nodes])\n    normalized_costs = {cand: distance_matrix[current_node][cand] / avg_distance for cand in candidates}\n\n    # Path momentum and direction (favor nodes in similar direction to previous movement)\n    momentum_scores = {cand: np.dot(np.array([distance_matrix[current_node][cand], distance_matrix[cand][destination_node]]),\n                                    np.array([1, -1])) for cand in candidates}\n\n    # Nearest neighbor clusters (identify clusters and favor nodes close to cluster centers)\n    cluster_centers = [unvisited_nodes[i] for i in np.random.choice(len(unvisited_nodes), min(3, len(unvisited_nodes)), replace=False)]\n    cluster_scores = {cand: cluster_score(cand, cluster_centers) for cand in candidates}\n\n    # Combine scores into a weighted evaluation function\n    combined_scores = {\n        cand: (0.3 * lookahead_scores[cand]) +\n              (0.2 * mst_costs[cand] / mst_cost_with_current) +\n              (0.1 * normalized_costs[cand]) +\n              (0.2 * momentum_scores[cand]) +\n              (0.2 * cluster_scores[cand])\n        for cand in candidates\n    }\n\n    # Beam search-like mechanism: maintain top-k candidates based on combined scores\n    beam_width = min(3, len(candidates))\n    top_candidates = sorted(combined_scores.keys(), key=lambda x: combined_scores[x])[:beam_width]\n\n    # Select the best candidate based on combined scores\n    next_node = min(top_candidates, key=lambda x: combined_scores[x])\n\n    return next_node",
          "objective": 8.44045,
          "other_inf": null,
          "metadata": {
               "operator": "i1",
               "wisdom_tips": [
                    "Design adaptive hybrid meta-heuristics synergistically fusing multiple search paradigms and dynamically tune operator parameters based on search stage or problem features",
                    "Employ machine learning or pattern recognition to mine deep problem structures and optimal solution patterns then use learned insights to intelligently bias towards promising search regions or constructive choices",
                    "Explore objective function engineering by introducing auxiliary or surrogate objectives or by dynamically adjusting weights to reshape the search landscape aiding escape from local optima or guiding diverse exploration"
               ],
               "attention_focus": "balancing local optimality with global search strategies",
               "generation_tendency": "balanced_search",
               "timestamp": 1748447674.758104
          }
     },
     {
          "algorithm": "Design an adaptive hybrid algorithm that integrates candidate sets, look-ahead evaluation, MST properties, edge cost normalization, path momentum, clustering, MCTS-inspired statistical reasoning, and beam search to iteratively select the next node for robust exploration of diverse high-quality paths.",
          "code": "import numpy as np\n\ndef select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):\n    def evaluate_candidate(node, current_path, unvisited):\n        # Simulate continuation from this node using lightweight Monte Carlo sampling\n        simulated_paths = []\n        for _ in range(5):  # Perform a few simulations\n            remaining = list(unvisited)\n            np.random.shuffle(remaining)\n            sim_path = current_path + [node] + remaining + [destination_node]\n            sim_cost = sum(distance_matrix[sim_path[i], sim_path[i+1]] for i in range(len(sim_path)-1))\n            simulated_paths.append(sim_cost)\n        return np.mean(simulated_paths)  # Average cost of simulated tours\n    \n    def cluster_score(node, clusters, current_path):\n        # Evaluate how well this node fits into identified clusters\n        cluster = next((c for c in clusters if node in c), [])\n        return len(cluster) / len(unvisited_nodes) if cluster else 0\n    \n    # Step 1: Identify candidate set (top 3 nearest neighbors)\n    candidates = sorted(unvisited_nodes, key=lambda x: distance_matrix[current_node, x])[:3]\n    \n    # Step 2: Build clusters of nearby nodes\n    distances = distance_matrix[unvisited_nodes][:, unvisited_nodes]\n    avg_distance = np.mean(distances)\n    clusters = [[n for n in unvisited_nodes if distance_matrix[node, n] < avg_distance] for node in candidates]\n    \n    # Step 3: Evaluate candidates using MCTS-inspired simulation and other metrics\n    current_path = [current_node]\n    scored_candidates = []\n    for candidate in candidates:\n        sim_score = evaluate_candidate(candidate, current_path, unvisited_nodes)\n        cluster_fit = cluster_score(candidate, clusters, current_path)\n        momentum = np.linalg.norm(np.array(current_path[-1]) - np.array(candidate)) if len(current_path) > 1 else 0\n        total_score = sim_score - cluster_fit - momentum  # Lower score is better\n        scored_candidates.append((candidate, total_score))\n    \n    # Step 4: Beam search-like selection of top candidate\n    scored_candidates.sort(key=lambda x: x[1])\n    next_node = scored_candidates[0][0]\n    return next_node",
          "objective": 10.5698,
          "other_inf": null,
          "metadata": {
               "operator": "i1",
               "wisdom_tips": [
                    "Design adaptive hybrid meta-heuristics synergistically fusing multiple search paradigms and dynamically tune operator parameters based on search stage or problem features",
                    "Employ machine learning or pattern recognition to mine deep problem structures and optimal solution patterns then use learned insights to intelligently bias towards promising search regions or constructive choices",
                    "Explore objective function engineering by introducing auxiliary or surrogate objectives or by dynamically adjusting weights to reshape the search landscape aiding escape from local optima or guiding diverse exploration"
               ],
               "attention_focus": "improving algorithm robustness across different problem instances",
               "generation_tendency": "balanced_search",
               "timestamp": 1748447674.7760708
          }
     }
]