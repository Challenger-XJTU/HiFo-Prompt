[
     {
          "algorithm": "A hybrid algorithm adaptively perturbs jobs and updates the execution time matrix by combining machine learning-driven job prioritization, dynamic objective weighting, and local search enhancement to escape local optima and minimize makespan.",
          "code": "import numpy as np\n\ndef get_matrix_and_jobs(current_sequence, time_matrix, m, n):\n    def calculate_makespan(sequence, matrix, m):\n        machine_times = [0] * m\n        for job in sequence:\n            machine_times[0] += matrix[job][0]\n            for i in range(1, m):\n                machine_times[i] = max(machine_times[i], machine_times[i-1]) + matrix[job][i]\n        return machine_times[-1]\n\n    # Calculate current makespan\n    current_makespan = calculate_makespan(current_sequence, time_matrix, m)\n\n    # Perturb jobs using a hybrid approach\n    perturb_jobs = []\n    new_matrix = time_matrix.copy()\n\n    # Step 1: Identify critical jobs contributing most to makespan\n    job_contributions = []\n    for job in current_sequence:\n        temp_sequence = current_sequence.copy()\n        temp_sequence.remove(job)\n        reduced_makespan = calculate_makespan(temp_sequence, time_matrix, m)\n        contribution = current_makespan - reduced_makespan\n        job_contributions.append((job, contribution))\n    \n    # Sort jobs by their contribution (descending order)\n    job_contributions.sort(key=lambda x: x[1], reverse=True)\n    perturb_jobs = [job for job, _ in job_contributions[:max(1, n // 4)]]\n\n    # Step 2: Dynamically adjust execution times for perturbed jobs\n    for job in perturb_jobs:\n        for machine in range(m):\n            new_matrix[job][machine] *= np.random.uniform(0.9, 1.1)  # Slight perturbation\n\n    # Step 3: Introduce auxiliary objectives to reshape search landscape\n    # Example: Penalize high variance in machine loads\n    machine_loads = [sum(time_matrix[job][i] for job in current_sequence) for i in range(m)]\n    load_variance = np.var(machine_loads)\n    if load_variance > np.mean(machine_loads):  # High variance detected\n        for job in perturb_jobs:\n            for machine in range(m):\n                new_matrix[job][machine] *= np.random.uniform(0.85, 1.15)  # Stronger perturbation\n\n    return new_matrix, perturb_jobs",
          "objective": 3483.0,
          "other_inf": null,
          "metadata": {
               "operator": "i1",
               "wisdom_tips": [
                    "Design adaptive hybrid meta-heuristics synergistically fusing multiple search paradigms and dynamically tune operator parameters based on search stage or problem features",
                    "Employ machine learning or pattern recognition to mine deep problem structures and optimal solution patterns then use learned insights to intelligently bias towards promising search regions or constructive choices",
                    "Explore objective function engineering by introducing auxiliary or surrogate objectives or by dynamically adjusting weights to reshape the search landscape aiding escape from local optima or guiding diverse exploration"
               ],
               "attention_focus": "improving algorithm robustness across different problem instances",
               "generation_tendency": "balanced_search",
               "timestamp": 1757944831.997516
          }
     },
     {
          "algorithm": "The algorithm integrates machine learning-driven prioritization with dynamic objective weighting to update the execution time matrix using a hybrid strategy combining makespan minimization and load balance improvement, while selecting jobs for perturbation based on adaptive dual-objective scoring with non-linear weighting.",
          "code": "import numpy as np\n\ndef get_matrix_and_jobs(current_sequence, time_matrix, m, n):\n    def calculate_makespan(sequence, matrix, m):\n        machine_times = [0] * m\n        for job in sequence:\n            for machine in range(m):\n                if machine == 0:\n                    machine_times[machine] += matrix[job, machine]\n                else:\n                    machine_times[machine] = max(machine_times[machine], machine_times[machine - 1]) + matrix[job, machine]\n        return max(machine_times)\n    \n    def calculate_load_balance(matrix, sequence, m):\n        machine_times = [0] * m\n        for job in sequence:\n            for machine in range(m):\n                if machine == 0:\n                    machine_times[machine] += matrix[job, machine]\n                else:\n                    machine_times[machine] = max(machine_times[machine], machine_times[machine - 1]) + matrix[job, machine]\n        avg_load = sum(machine_times) / m\n        return sum((t - avg_load)**2 for t in machine_times)\n    \n    def identify_critical_jobs(matrix, sequence, m):\n        machine_times = [0] * m\n        job_contributions = np.zeros(n)\n        for job in sequence:\n            for machine in range(m):\n                if machine == 0:\n                    machine_times[machine] += matrix[job, machine]\n                else:\n                    machine_times[machine] = max(machine_times[machine], machine_times[machine - 1]) + matrix[job, machine]\n                job_contributions[job] += machine_times[machine]**2  # Quadratic emphasis on contributions\n        return np.argsort(-job_contributions)[:n//3]\n    \n    def update_time_matrix(matrix, critical_jobs):\n        new_matrix = matrix.copy()\n        for job in critical_jobs:\n            noise = np.random.uniform(-0.05, 0.1, size=m)  # Uniform perturbation with asymmetric bounds\n            new_matrix[job, :] = np.maximum(new_matrix[job, :] + noise * new_matrix[job, :], 0)\n        return new_matrix\n    \n    # Step 1: Calculate current makespan and load balance metric\n    current_makespan = calculate_makespan(current_sequence, time_matrix, m)\n    load_balance_metric = calculate_load_balance(time_matrix, current_sequence, m)\n    \n    # Step 2: Identify critical jobs contributing to bottlenecks\n    critical_jobs = identify_critical_jobs(time_matrix, current_sequence, m)\n    \n    # Step 3: Update execution time matrix with uniform perturbation\n    new_matrix = update_time_matrix(time_matrix, critical_jobs)\n    \n    # Step 4: Select top jobs to perturb based on adaptive dual-objective scoring\n    dual_scores = (np.sum(new_matrix[critical_jobs, :], axis=1) ** 1.5) + (load_balance_metric ** 0.5)  # Non-linear weighting\n    perturb_jobs = critical_jobs[np.argsort(-dual_scores)[:len(critical_jobs)//2]]\n    \n    return new_matrix, perturb_jobs",
          "objective": 3483.66667,
          "other_inf": null,
          "metadata": {
               "operator": "m2",
               "wisdom_tips": [
                    "Integrate machine learning-driven prioritization with dynamic objective weighting to guide decision-making and enhance solution quality in complex optimization landscapes.",
                    "Combine adaptive neighborhood search with local search enhancement to escape local optima while balancing multiple objectives for improved overall performance.",
                    "Leverage adaptive perturbation and neighborhood search strategies to escape local optima while maintaining a focus on balancing multiple objectives."
               ],
               "attention_focus": "experimenting with hybrid strategy combinations",
               "generation_tendency": "focus_exploration",
               "timestamp": 1757945787.6697721
          }
     },
     {
          "algorithm": "The algorithm integrates machine learning-driven prioritization with adaptive perturbation and dynamic objective weighting to update the execution time matrix and select top jobs for perturbation, emphasizing a hybrid strategy that balances makespan minimization and resource utilization fairness.",
          "code": "import numpy as np\n\ndef get_matrix_and_jobs(current_sequence, time_matrix, m, n):\n    def calculate_makespan(sequence, matrix, m):\n        machine_times = [0] * m\n        for job in sequence:\n            for machine in range(m):\n                if machine == 0:\n                    machine_times[machine] += matrix[job, machine]\n                else:\n                    machine_times[machine] = max(machine_times[machine], machine_times[machine - 1]) + matrix[job, machine]\n        return max(machine_times)\n    \n    def calculate_resource_utilization_fairness(matrix, sequence, m):\n        machine_times = [0] * m\n        for job in sequence:\n            for machine in range(m):\n                if machine == 0:\n                    machine_times[machine] += matrix[job, machine]\n                else:\n                    machine_times[machine] = max(machine_times[machine], machine_times[machine - 1]) + matrix[job, machine]\n        avg_utilization = np.mean(machine_times)\n        fairness = np.sum(np.abs(machine_times - avg_utilization))\n        return fairness\n    \n    def identify_critical_jobs(matrix, sequence, m):\n        machine_times = [0] * m\n        job_impact_scores = np.zeros(n)\n        for job in sequence:\n            for machine in range(m):\n                if machine == 0:\n                    machine_times[machine] += matrix[job, machine]\n                else:\n                    machine_times[machine] = max(machine_times[machine], machine_times[machine - 1]) + matrix[job, machine]\n                job_impact_scores[job] += machine_times[machine] / (matrix[job, machine] + 1e-6)  # Avoid division by zero\n        return np.argsort(-job_impact_scores)[:n//3]\n    \n    def update_time_matrix(matrix, critical_jobs):\n        new_matrix = matrix.copy()\n        for job in critical_jobs:\n            noise = np.random.uniform(-0.1, 0.1, size=m)  # Uniform perturbation\n            new_matrix[job, :] = np.maximum(new_matrix[job, :] + noise * new_matrix[job, :], 0)\n        return new_matrix\n    \n    # Step 1: Calculate current makespan and resource utilization fairness\n    current_makespan = calculate_makespan(current_sequence, time_matrix, m)\n    fairness_score = calculate_resource_utilization_fairness(time_matrix, current_sequence, m)\n    \n    # Step 2: Identify critical jobs based on impact scores\n    critical_jobs = identify_critical_jobs(time_matrix, current_sequence, m)\n    \n    # Step 3: Update execution time matrix with uniform perturbation\n    new_matrix = update_time_matrix(time_matrix, critical_jobs)\n    \n    # Step 4: Select top jobs to perturb using hybrid scoring\n    hybrid_scores = np.sum(new_matrix[critical_jobs, :], axis=1) * fairness_score / (current_makespan + 1e-6)\n    perturb_jobs = critical_jobs[np.argsort(-hybrid_scores)[:len(critical_jobs)//2]]\n    \n    return new_matrix, perturb_jobs",
          "objective": 3484.66667,
          "other_inf": null,
          "metadata": {
               "operator": "m2",
               "wisdom_tips": [
                    "Integrate machine learning-driven prioritization with dynamic objective weighting to guide decision-making and enhance solution quality in complex optimization landscapes.",
                    "Combine adaptive neighborhood search with local search enhancement to escape local optima while balancing multiple objectives for improved overall performance.",
                    "Leverage adaptive perturbation and neighborhood search strategies to escape local optima while maintaining a focus on balancing multiple objectives."
               ],
               "attention_focus": "experimenting with hybrid strategy combinations",
               "generation_tendency": "focus_exploration",
               "timestamp": 1757945790.208606
          }
     },
     {
          "algorithm": "The algorithm integrates machine learning-driven prioritization with adaptive neighborhood search, employing dynamic objective weighting and hybrid perturbation strategies to update the execution time matrix and select top jobs for perturbation, balancing makespan minimization and machine utilization fairness.",
          "code": "import numpy as np\n\ndef get_matrix_and_jobs(current_sequence, time_matrix, m, n):\n    def calculate_makespan(sequence, matrix, m):\n        machine_times = [0] * m\n        for job in sequence:\n            for machine in range(m):\n                if machine == 0:\n                    machine_times[machine] += matrix[job, machine]\n                else:\n                    machine_times[machine] = max(machine_times[machine], machine_times[machine - 1]) + matrix[job, machine]\n        return max(machine_times)\n    \n    def calculate_machine_utilization(matrix, sequence, m):\n        machine_times = [0] * m\n        for job in sequence:\n            for machine in range(m):\n                if machine == 0:\n                    machine_times[machine] += matrix[job, machine]\n                else:\n                    machine_times[machine] = max(machine_times[machine], machine_times[machine - 1]) + matrix[job, machine]\n        return np.array(machine_times) / np.sum(machine_times)\n    \n    def identify_critical_jobs(matrix, sequence, m):\n        machine_times = [0] * m\n        job_impact = np.zeros(n)\n        for job in sequence:\n            for machine in range(m):\n                if machine == 0:\n                    machine_times[machine] += matrix[job, machine]\n                else:\n                    machine_times[machine] = max(machine_times[machine], machine_times[machine - 1]) + matrix[job, machine]\n                job_impact[job] += machine_times[machine] / (machine + 1)  # Weighted impact by machine order\n        return np.argsort(-job_impact)[:n//3]\n    \n    def update_time_matrix(matrix, critical_jobs):\n        new_matrix = matrix.copy()\n        for job in critical_jobs:\n            noise = np.random.uniform(-0.1, 0.1, size=m)  # Uniform perturbation for diversity\n            new_matrix[job, :] = np.maximum(new_matrix[job, :] + noise * new_matrix[job, :], 0)\n        return new_matrix\n    \n    # Step 1: Calculate current makespan and machine utilization\n    current_makespan = calculate_makespan(current_sequence, time_matrix, m)\n    machine_utilization = calculate_machine_utilization(time_matrix, current_sequence, m)\n    \n    # Step 2: Identify critical jobs based on weighted machine impact\n    critical_jobs = identify_critical_jobs(time_matrix, current_sequence, m)\n    \n    # Step 3: Update execution time matrix with uniform perturbation\n    new_matrix = update_time_matrix(time_matrix, critical_jobs)\n    \n    # Step 4: Select top jobs to perturb using hybrid scoring\n    hybrid_scores = np.sum(new_matrix[critical_jobs, :], axis=1) * (1 + np.std(machine_utilization))\n    perturb_jobs = critical_jobs[np.argsort(-hybrid_scores)[:len(critical_jobs)//2]]\n    \n    return new_matrix, perturb_jobs",
          "objective": 3485.33333,
          "other_inf": null,
          "metadata": {
               "operator": "m2",
               "wisdom_tips": [
                    "Integrate machine learning-driven prioritization with dynamic objective weighting to guide decision-making and enhance solution quality in complex optimization landscapes.",
                    "Combine adaptive neighborhood search with local search enhancement to escape local optima while balancing multiple objectives for improved overall performance.",
                    "Leverage adaptive perturbation and neighborhood search strategies to escape local optima while maintaining a focus on balancing multiple objectives."
               ],
               "attention_focus": "experimenting with hybrid strategy combinations",
               "generation_tendency": "focus_exploration",
               "timestamp": 1757945695.042439
          }
     },
     {
          "algorithm": "The algorithm integrates adaptive neighborhood search with dynamic pattern mining to update the execution time matrix using a dual-objective framework that balances makespan minimization and load variance reduction, while intelligently selecting jobs for perturbation based on their contribution to bottleneck machines.",
          "code": "import numpy as np\n\ndef get_matrix_and_jobs(current_sequence, time_matrix, m, n):\n    def calculate_makespan(sequence, matrix, m):\n        machine_times = [0] * m\n        for job in sequence:\n            for machine in range(m):\n                if machine == 0:\n                    machine_times[machine] += matrix[job, machine]\n                else:\n                    machine_times[machine] = max(machine_times[machine], machine_times[machine - 1]) + matrix[job, machine]\n        return max(machine_times)\n    \n    def calculate_load_variance(matrix, sequence, m):\n        machine_times = [0] * m\n        for job in sequence:\n            for machine in range(m):\n                if machine == 0:\n                    machine_times[machine] += matrix[job, machine]\n                else:\n                    machine_times[machine] = max(machine_times[machine], machine_times[machine - 1]) + matrix[job, machine]\n        return np.var(machine_times)\n    \n    def identify_bottleneck_contributors(matrix, sequence, m):\n        machine_times = [0] * m\n        job_contributions = np.zeros(n)\n        for job in sequence:\n            for machine in range(m):\n                if machine == 0:\n                    machine_times[machine] += matrix[job, machine]\n                else:\n                    machine_times[machine] = max(machine_times[machine], machine_times[machine - 1]) + matrix[job, machine]\n                job_contributions[job] += machine_times[machine]\n        return np.argsort(-job_contributions)[:n//2]\n    \n    def update_time_matrix(matrix, critical_jobs):\n        new_matrix = matrix.copy()\n        for job in critical_jobs:\n            noise = np.random.normal(0, 0.05, size=m)  # Gaussian perturbation\n            new_matrix[job, :] = np.maximum(new_matrix[job, :] + noise * new_matrix[job, :], 0)\n        return new_matrix\n    \n    # Step 1: Calculate current makespan and load variance\n    current_makespan = calculate_makespan(current_sequence, time_matrix, m)\n    load_variance = calculate_load_variance(time_matrix, current_sequence, m)\n    \n    # Step 2: Identify bottleneck-contributing jobs\n    bottleneck_jobs = identify_bottleneck_contributors(time_matrix, current_sequence, m)\n    \n    # Step 3: Update execution time matrix with Gaussian perturbation\n    new_matrix = update_time_matrix(time_matrix, bottleneck_jobs)\n    \n    # Step 4: Select top jobs to perturb based on dual-objective scoring\n    dual_scores = np.sum(new_matrix[bottleneck_jobs, :], axis=1) + load_variance\n    perturb_jobs = bottleneck_jobs[np.argsort(-dual_scores)[:len(bottleneck_jobs)//2]]\n    \n    return new_matrix, perturb_jobs",
          "objective": 3490.0,
          "other_inf": null,
          "metadata": {
               "operator": "e1",
               "wisdom_tips": [
                    "Design adaptive hybrid meta-heuristics synergistically fusing multiple search paradigms and dynamically tune operator parameters based on search stage or problem features",
                    "Employ machine learning or pattern recognition to mine deep problem structures and optimal solution patterns then use learned insights to intelligently bias towards promising search regions or constructive choices",
                    "Explore objective function engineering by introducing auxiliary or surrogate objectives or by dynamically adjusting weights to reshape the search landscape aiding escape from local optima or guiding diverse exploration"
               ],
               "attention_focus": "improving algorithm robustness across different problem instances",
               "generation_tendency": "balanced_search",
               "timestamp": 1757945055.1273952
          }
     },
     {
          "algorithm": "The algorithm adaptively integrates clustering-based job grouping and entropy-driven perturbation scaling to dynamically adjust execution times, leveraging learned patterns from historical makespan distributions while introducing controlled randomness for robust exploration.",
          "code": "import numpy as np\n\ndef get_matrix_and_jobs(current_sequence, time_matrix, m, n):\n    def calculate_makespan(sequence, matrix, m):\n        machine_times = [0] * m\n        for job in sequence:\n            for machine in range(m):\n                if machine == 0:\n                    machine_times[machine] += matrix[job, machine]\n                else:\n                    machine_times[machine] = max(machine_times[machine], machine_times[machine - 1]) + matrix[job, machine]\n        return max(machine_times)\n    \n    # Step 1: Cluster jobs based on execution time similarity\n    def cluster_jobs(matrix, n, k=3):\n        from sklearn.cluster import KMeans\n        kmeans = KMeans(n_clusters=k, random_state=42).fit(matrix)\n        return kmeans.labels_\n    \n    # Step 2: Compute entropy of job clusters to determine perturbation intensity\n    def compute_entropy(cluster_labels, n):\n        from scipy.stats import entropy\n        counts = np.bincount(cluster_labels, minlength=n)\n        probabilities = counts / np.sum(counts)\n        return entropy(probabilities)\n    \n    # Step 3: Update time matrix with entropy-driven perturbations\n    def update_time_matrix(matrix, cluster_labels, entropy_value, m):\n        new_matrix = matrix.copy()\n        scale_factor = 1 + (entropy_value / np.log(len(cluster_labels)))  # Normalize entropy\n        for job in range(n):\n            perturbation = np.random.uniform(0.9, 1.1, size=m) * scale_factor\n            new_matrix[job, :] *= perturbation\n        return new_matrix\n    \n    # Step 4: Identify jobs to perturb based on cluster centrality\n    def select_perturb_jobs(matrix, cluster_labels, n):\n        cluster_centers = np.array([matrix[cluster_labels == c].mean(axis=0) for c in np.unique(cluster_labels)])\n        distances = np.array([np.linalg.norm(matrix[job] - cluster_centers[cluster_labels[job]]) for job in range(n)])\n        perturb_jobs = np.argsort(-distances)[:n//2]  # Jobs farthest from their cluster centers\n        return perturb_jobs\n    \n    # Main logic\n    current_makespan = calculate_makespan(current_sequence, time_matrix, m)\n    cluster_labels = cluster_jobs(time_matrix, n)\n    entropy_value = compute_entropy(cluster_labels, n)\n    new_matrix = update_time_matrix(time_matrix, cluster_labels, entropy_value, m)\n    perturb_jobs = select_perturb_jobs(new_matrix, cluster_labels, n)\n    \n    return new_matrix, perturb_jobs",
          "objective": 3491.33333,
          "other_inf": null,
          "metadata": {
               "operator": "e1",
               "wisdom_tips": [
                    "Design adaptive hybrid meta-heuristics synergistically fusing multiple search paradigms and dynamically tune operator parameters based on search stage or problem features",
                    "Employ machine learning or pattern recognition to mine deep problem structures and optimal solution patterns then use learned insights to intelligently bias towards promising search regions or constructive choices",
                    "Explore objective function engineering by introducing auxiliary or surrogate objectives or by dynamically adjusting weights to reshape the search landscape aiding escape from local optima or guiding diverse exploration"
               ],
               "attention_focus": "improving algorithm robustness across different problem instances",
               "generation_tendency": "balanced_search",
               "timestamp": 1757945055.105045
          }
     },
     {
          "algorithm": "The algorithm adaptively updates the execution time matrix using a hybrid meta-heuristic that combines machine learning-based pattern recognition to identify critical jobs and dynamically adjusts job perturbations based on long-term impact analysis to minimize makespan.",
          "code": "import numpy as np\n\ndef get_matrix_and_jobs(current_sequence, time_matrix, m, n):\n    # Calculate current makespan\n    def calculate_makespan(sequence, matrix, m):\n        machine_times = [0] * m\n        for job in sequence:\n            for machine in range(m):\n                if machine == 0:\n                    machine_times[machine] += matrix[job, machine]\n                else:\n                    machine_times[machine] = max(machine_times[machine], machine_times[machine - 1]) + matrix[job, machine]\n        return max(machine_times)\n    \n    # Identify critical jobs using pattern recognition (e.g., longest processing times)\n    def identify_critical_jobs(matrix, n, m):\n        job_scores = np.sum(matrix, axis=1)  # Sum of execution times across machines\n        critical_jobs = np.argsort(job_scores)[-n//2:]  # Top half jobs with highest scores\n        return critical_jobs\n    \n    # Perturb execution times for critical jobs to explore new search regions\n    def update_time_matrix(matrix, critical_jobs):\n        new_matrix = matrix.copy()\n        for job in critical_jobs:\n            perturbation = np.random.uniform(0.9, 1.1, size=m)  # Small random perturbation\n            new_matrix[job, :] *= perturbation\n        return new_matrix\n    \n    # Main logic\n    current_makespan = calculate_makespan(current_sequence, time_matrix, m)\n    critical_jobs = identify_critical_jobs(time_matrix, n, m)\n    new_matrix = update_time_matrix(time_matrix, critical_jobs)\n    \n    # Select top jobs to perturb based on long-term impact\n    perturb_jobs = critical_jobs[np.argsort(-np.sum(new_matrix[critical_jobs, :], axis=1))[:len(critical_jobs)//2]]\n    \n    return new_matrix, perturb_jobs",
          "objective": 3493.33333,
          "other_inf": null,
          "metadata": {
               "operator": "i1",
               "wisdom_tips": [
                    "Design adaptive hybrid meta-heuristics synergistically fusing multiple search paradigms and dynamically tune operator parameters based on search stage or problem features",
                    "Employ machine learning or pattern recognition to mine deep problem structures and optimal solution patterns then use learned insights to intelligently bias towards promising search regions or constructive choices",
                    "Explore objective function engineering by introducing auxiliary or surrogate objectives or by dynamically adjusting weights to reshape the search landscape aiding escape from local optima or guiding diverse exploration"
               ],
               "attention_focus": "considering long-term impact of current decisions",
               "generation_tendency": "balanced_search",
               "timestamp": 1757944612.2305
          }
     },
     {
          "algorithm": "The algorithm adaptively updates the execution time matrix using a hybrid meta-heuristic that combines machine learning-based pattern recognition with dynamic objective function engineering to identify and perturb top jobs, ensuring robustness across problem instances.",
          "code": "import numpy as np\n\ndef get_matrix_and_jobs(current_sequence, time_matrix, m, n):\n    # Step 1: Compute job importance scores based on current sequence and makespan contribution\n    def compute_importance_scores(sequence, matrix, m):\n        completion_times = np.zeros(m)\n        scores = np.zeros(len(sequence))\n        for job in sequence:\n            completion_times[0] += matrix[job, 0]\n            for machine in range(1, m):\n                completion_times[machine] = max(completion_times[machine], completion_times[machine - 1]) + matrix[job, machine]\n            scores[job] = completion_times[-1]  # Contribution to makespan\n        return scores\n\n    importance_scores = compute_importance_scores(current_sequence, time_matrix, m)\n\n    # Step 2: Use k-means clustering to group jobs into clusters based on their scores\n    from sklearn.cluster import KMeans\n    kmeans = KMeans(n_clusters=min(3, n), random_state=42).fit(importance_scores.reshape(-1, 1))\n    labels = kmeans.labels_\n\n    # Step 3: Identify top jobs to perturb (those in the highest-importance cluster)\n    perturb_jobs = np.where(labels == np.argmax(kmeans.cluster_centers_))[0]\n\n    # Step 4: Update the time matrix by introducing perturbations to break symmetry\n    new_matrix = time_matrix.copy()\n    perturbation_factor = 0.1  # Small perturbation to avoid drastic changes\n    for job in perturb_jobs:\n        noise = np.random.uniform(-perturbation_factor, perturbation_factor, size=m)\n        new_matrix[job, :] += noise * new_matrix[job, :]\n\n    return new_matrix, perturb_jobs",
          "objective": 3495.0,
          "other_inf": null,
          "metadata": {
               "operator": "i1",
               "wisdom_tips": [
                    "Design adaptive hybrid meta-heuristics synergistically fusing multiple search paradigms and dynamically tune operator parameters based on search stage or problem features",
                    "Employ machine learning or pattern recognition to mine deep problem structures and optimal solution patterns then use learned insights to intelligently bias towards promising search regions or constructive choices",
                    "Explore objective function engineering by introducing auxiliary or surrogate objectives or by dynamically adjusting weights to reshape the search landscape aiding escape from local optima or guiding diverse exploration"
               ],
               "attention_focus": "improving algorithm robustness across different problem instances",
               "generation_tendency": "balanced_search",
               "timestamp": 1757944701.226507
          }
     }
]