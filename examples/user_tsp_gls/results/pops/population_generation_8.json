[
     {
          "algorithm": "```",
          "code": "import numpy as np\n\ndef update_edge_distance(edge_distance, local_opt_tour, edge_n_used):\n    num_nodes = edge_distance.shape[0]\n    updated_edge_distance = edge_distance.copy()\n    \n    # Simplified entropy-driven penalty mechanism\n    edge_usage_prob = edge_n_used / (np.sum(edge_n_used) + 1e-8)\n    edge_entropy = -edge_usage_prob * np.log(edge_usage_prob + 1e-8)\n    normalized_entropy = edge_entropy / (np.max(edge_entropy) + 1e-8)\n    \n    for i in range(len(local_opt_tour) - 1):\n        u, v = local_opt_tour[i], local_opt_tour[i + 1]\n        entropy_penalty = 1 - normalized_entropy[u, v]\n        updated_edge_distance[u, v] += entropy_penalty\n        updated_edge_distance[v, u] = updated_edge_distance[u, v]\n    \n    # Simplified topology-aware stochastic scaling\n    global_entropy = np.mean(normalized_entropy)\n    if global_entropy < 0.5:\n        stochastic_perturbation = np.random.uniform(0, 0.1, (num_nodes, num_nodes))\n        updated_edge_distance += stochastic_perturbation\n    \n    # Ensure no negative distances\n    updated_edge_distance = np.maximum(updated_edge_distance, 0)\n    \n    return updated_edge_distance",
          "objective": 0.1522,
          "other_inf": null,
          "metadata": {
               "operator": "m3",
               "wisdom_tips": [
                    "Leverage entropy-driven mechanisms and cumulative frequency adjustments to preserve solution diversity and enhance convergence efficiency.",
                    "Leverage entropy-driven strategies and adaptive scaling to maintain solution diversity and enhance the robustness of the optimization process.",
                    "Leverage entropy-driven and topology-aware strategies to iteratively refine the search space for improved global optimization."
               ],
               "attention_focus": "experimenting with hybrid strategy combinations",
               "generation_tendency": "focus_exploration",
               "timestamp": 1757181951.39392
          }
     },
     {
          "algorithm": "Design a hybrid meta-heuristic that integrates topology-aware penalties, stochastic perturbations with adaptive scaling, and cumulative frequency balancing to iteratively reshape the search landscape while enhancing exploration and exploitation through non-linear feedback mechanisms.",
          "code": "import numpy as np\n\ndef update_edge_distance(edge_distance, local_opt_tour, edge_n_used):\n    num_nodes = edge_distance.shape[0]\n    updated_edge_distance = edge_distance.copy()\n    \n    # Parameter initialization\n    topology_penalty_factor = 1.5\n    stochastic_scale_min, stochastic_scale_max = 0.1, 0.9\n    frequency_balance_weight = 0.6\n    \n    # Compute topology-aware penalties based on global connectivity\n    for i in range(len(local_opt_tour) - 1):\n        u, v = local_opt_tour[i], local_opt_tour[i + 1]\n        global_connectivity = np.sum(edge_n_used[u, :]) + np.sum(edge_n_used[v, :])\n        penalty = topology_penalty_factor * (1 + np.exp(-global_connectivity / (np.mean(edge_n_used) + 1e-8)))\n        updated_edge_distance[u, v] += penalty\n        updated_edge_distance[v, u] = updated_edge_distance[u, v]\n    \n    # Apply stochastic perturbations with adaptive scaling\n    stochastic_scale = stochastic_scale_min + (stochastic_scale_max - stochastic_scale_min) * np.random.rand(num_nodes, num_nodes)\n    stochastic_perturbation = stochastic_scale * (np.max(edge_n_used) - edge_n_used) / (np.max(edge_n_used) + 1e-8)\n    updated_edge_distance += stochastic_perturbation\n    \n    # Introduce cumulative frequency balancing adjustments\n    frequency_balance = frequency_balance_weight * (edge_n_used - np.mean(edge_n_used)) / (np.std(edge_n_used) + 1e-8)\n    frequency_balance = np.clip(frequency_balance, -np.inf, 0)  # Ensure only downward adjustments\n    updated_edge_distance += frequency_balance\n    \n    # Ensure no negative distances\n    updated_edge_distance = np.maximum(updated_edge_distance, 0)\n    \n    return updated_edge_distance",
          "objective": 0.15311,
          "other_inf": null,
          "metadata": {
               "operator": "m1",
               "wisdom_tips": [
                    "Introduce topology-aware penalties to dynamically reshape the search landscape, ensuring decisions reflect both local structure and global connectivity.",
                    "Incorporate stochastic perturbations with adaptive scaling to balance exploration and exploitation, enhancing robustness across diverse problem instances.",
                    "Integrate dynamic penalties and adaptive scaling to reshape the search landscape iteratively, enhancing both exploration and exploitation."
               ],
               "attention_focus": "experimenting with hybrid strategy combinations",
               "generation_tendency": "focus_exploration",
               "timestamp": 1757181137.06735
          }
     },
     {
          "algorithm": "A hybrid strategy combining entropy-driven edge penalties and topology-aware stochastic scaling to reshape the search space dynamically while preserving solution diversity.",
          "code": "import numpy as np\n\ndef update_edge_distance(edge_distance, local_opt_tour, edge_n_used):\n    num_nodes = edge_distance.shape[0]\n    updated_edge_distance = edge_distance.copy()\n    \n    # Parameter initialization for hybrid strategy\n    entropy_penalty_weight = 0.7\n    stochastic_scaling_factor = 0.2\n    global_entropy_threshold = 0.5\n    \n    # Compute entropy-based penalties to discourage overused edges\n    edge_usage_prob = edge_n_used / (np.sum(edge_n_used) + 1e-8)\n    edge_entropy = -edge_usage_prob * np.log(edge_usage_prob + 1e-8)\n    normalized_entropy = edge_entropy / (np.max(edge_entropy) + 1e-8)\n    \n    for i in range(len(local_opt_tour) - 1):\n        u, v = local_opt_tour[i], local_opt_tour[i + 1]\n        entropy_penalty = entropy_penalty_weight * (1 - normalized_entropy[u, v])\n        updated_edge_distance[u, v] += entropy_penalty\n        updated_edge_distance[v, u] = updated_edge_distance[u, v]\n    \n    # Apply topology-aware stochastic scaling based on global entropy\n    global_entropy = np.mean(normalized_entropy)\n    if global_entropy < global_entropy_threshold:\n        stochastic_perturbation = np.random.normal(0, stochastic_scaling_factor, (num_nodes, num_nodes))\n        stochastic_perturbation = np.where(stochastic_perturbation > 0, stochastic_perturbation, 0)\n        updated_edge_distance += stochastic_perturbation\n    \n    # Ensure no negative distances\n    updated_edge_distance = np.maximum(updated_edge_distance, 0)\n    \n    return updated_edge_distance",
          "objective": 0.16825,
          "other_inf": null,
          "metadata": {
               "operator": "e1",
               "wisdom_tips": [
                    "Integrate topology-aware feedback to dynamically reshape the search space and enhance exploration-exploitation trade-offs.",
                    "Leverage stochastic perturbations with adaptive scaling to maintain diversity and adaptability in solution refinement.",
                    "Leverage entropy-driven or stochastic perturbations to enhance global search capabilities while preserving structural integrity in solution refinement."
               ],
               "attention_focus": "experimenting with hybrid strategy combinations",
               "generation_tendency": "focus_exploration",
               "timestamp": 1757181685.049485
          }
     },
     {
          "algorithm": "Design a hybrid meta-heuristic that integrates entropy-driven penalties, topology-aware stochastic scaling, and non-linear frequency attenuation to dynamically reshape the search landscape while balancing exploration and exploitation through adaptive feedback mechanisms.",
          "code": "import numpy as np\n\ndef update_edge_distance(edge_distance, local_opt_tour, edge_n_used):\n    num_nodes = edge_distance.shape[0]\n    updated_edge_distance = edge_distance.copy()\n    \n    # Parameter initialization\n    entropy_penalty_factor = 2.0\n    stochastic_scale_min, stochastic_scale_max = 0.2, 1.2\n    frequency_attenuation_weight = 0.8\n    \n    # Compute entropy-driven penalties based on historical usage patterns\n    for i in range(len(local_opt_tour) - 1):\n        u, v = local_opt_tour[i], local_opt_tour[i + 1]\n        usage_entropy = -np.sum((edge_n_used[u, :] / (np.sum(edge_n_used[u, :]) + 1e-8)) * \n                                np.log(edge_n_used[u, :] / (np.sum(edge_n_used[u, :]) + 1e-8) + 1e-8))\n        penalty = entropy_penalty_factor * (1 + np.exp(-usage_entropy / (np.mean(edge_n_used) + 1e-8)))\n        updated_edge_distance[u, v] += penalty\n        updated_edge_distance[v, u] = updated_edge_distance[u, v]\n    \n    # Apply topology-aware stochastic scaling with adaptive adjustment\n    stochastic_scale = stochastic_scale_min + (stochastic_scale_max - stochastic_scale_min) * np.random.rand(num_nodes, num_nodes)\n    stochastic_perturbation = stochastic_scale * (np.max(edge_n_used) - edge_n_used) / (np.max(edge_n_used) + 1e-8)\n    updated_edge_distance += stochastic_perturbation\n    \n    # Introduce non-linear frequency attenuation adjustments\n    frequency_attenuation = frequency_attenuation_weight * np.tanh((edge_n_used - np.mean(edge_n_used)) / (np.std(edge_n_used) + 1e-8))\n    frequency_attenuation = np.clip(frequency_attenuation, -np.inf, 0)  # Ensure only downward adjustments\n    updated_edge_distance += frequency_attenuation\n    \n    # Ensure no negative distances\n    updated_edge_distance = np.maximum(updated_edge_distance, 0)\n    \n    return updated_edge_distance",
          "objective": 0.19834,
          "other_inf": null,
          "metadata": {
               "operator": "m2",
               "wisdom_tips": [
                    "Integrate entropy-driven penalties to dynamically reshape the search landscape, balancing exploration and exploitation based on historical usage patterns.",
                    "Use topology-aware stochastic scaling to adaptively adjust decision-making, ensuring robustness to varying problem structures and enhancing convergence efficiency.",
                    "Leverage topology-aware stochastic scaling to adaptively adjust solution components, enhancing robustness to problem-specific structures and variability."
               ],
               "attention_focus": "experimenting with hybrid strategy combinations",
               "generation_tendency": "focus_exploration",
               "timestamp": 1757182221.3624651
          }
     },
     {
          "algorithm": "```",
          "code": "import numpy as np\n\ndef update_edge_distance(edge_distance, local_opt_tour, edge_n_used):\n    num_nodes = edge_distance.shape[0]\n    updated_edge_distance = edge_distance.copy()\n    \n    # Simplified parameter initialization\n    topology_penalty_factor = 1.2\n    stochastic_scale_range = (0.2, 0.8)\n    \n    # Simplified topology-aware penalties\n    for i in range(len(local_opt_tour) - 1):\n        u, v = local_opt_tour[i], local_opt_tour[i + 1]\n        global_connectivity = np.sum(edge_n_used[u, :]) + np.sum(edge_n_used[v, :])\n        penalty = topology_penalty_factor * (1 + np.exp(-global_connectivity / (np.mean(edge_n_used) + 1e-8)))\n        updated_edge_distance[u, v] += penalty\n        updated_edge_distance[v, u] = updated_edge_distance[u, v]\n    \n    # Simplified stochastic perturbations with adaptive scaling\n    stochastic_scale = np.random.uniform(*stochastic_scale_range, size=(num_nodes, num_nodes))\n    stochastic_perturbation = stochastic_scale * (1 - edge_n_used / (np.max(edge_n_used) + 1e-8))\n    updated_edge_distance += stochastic_perturbation\n    \n    # Simplified frequency balancing adjustments\n    frequency_balance = (edge_n_used - np.mean(edge_n_used)) / (np.std(edge_n_used) + 1e-8)\n    frequency_balance = np.clip(frequency_balance, -np.inf, 0)  # Ensure only downward adjustments\n    updated_edge_distance += 0.5 * frequency_balance\n    \n    # Ensure no negative distances\n    updated_edge_distance = np.maximum(updated_edge_distance, 0)\n    \n    return updated_edge_distance",
          "objective": 0.20371,
          "other_inf": null,
          "metadata": {
               "operator": "m3",
               "wisdom_tips": [
                    "Leverage topology-aware stochastic scaling to adaptively adjust solution components, enhancing robustness to problem-specific structures and variability.",
                    "Integrate entropy-driven penalties to dynamically reshape the search landscape, balancing exploration and exploitation by penalizing overused components while preserving diversity.",
                    "Leverage topology-aware adaptive scaling to iteratively adjust decision-making based on global structural insights, enhancing both local refinements and broader search efficiency."
               ],
               "attention_focus": "experimenting with hybrid strategy combinations",
               "generation_tendency": "focus_exploration",
               "timestamp": 1757182351.55717
          }
     },
     {
          "algorithm": "```",
          "code": "import numpy as np\n\ndef update_edge_distance(edge_distance, local_opt_tour, edge_n_used):\n    num_nodes = edge_distance.shape[0]\n    updated_edge_distance = edge_distance.copy()\n    \n    # Simplified parameter initialization\n    topology_penalty_weight = 0.5\n    smoothing_factor = 0.1\n    \n    # Simplified topology-aware penalties based on local optimal tour\n    for i in range(len(local_opt_tour) - 1):\n        u, v = local_opt_tour[i], local_opt_tour[i + 1]\n        penalty = topology_penalty_weight * (1 + np.exp(-edge_n_used[u, v]))\n        updated_edge_distance[u, v] += penalty\n        updated_edge_distance[v, u] = updated_edge_distance[u, v]\n    \n    # Simplified stochastic perturbations with adaptive scaling\n    gradient_perturbation = np.random.uniform(-smoothing_factor, smoothing_factor, (num_nodes, num_nodes))\n    updated_edge_distance += gradient_perturbation\n    \n    # Ensure no negative distances\n    updated_edge_distance = np.maximum(updated_edge_distance, 0)\n    \n    return updated_edge_distance",
          "objective": 0.21461,
          "other_inf": null,
          "metadata": {
               "operator": "m3",
               "wisdom_tips": [
                    "Design adaptive hybrid meta-heuristics synergistically fusing multiple search paradigms and dynamically tune operator parameters based on search stage or problem features",
                    "Employ machine learning or pattern recognition to mine deep problem structures and optimal solution patterns then use learned insights to intelligently bias towards promising search regions or constructive choices",
                    "Explore objective function engineering by introducing auxiliary or surrogate objectives or by dynamically adjusting weights to reshape the search landscape aiding escape from local optima or guiding diverse exploration"
               ],
               "attention_focus": "experimenting with hybrid strategy combinations",
               "generation_tendency": "focus_exploration",
               "timestamp": 1757180120.721326
          }
     },
     {
          "algorithm": "Design a hybrid meta-heuristic that combines topology-aware penalties, stochastic perturbations with adaptive scaling, and edge frequency normalization to iteratively reshape the search landscape while balancing exploration and exploitation.",
          "code": "import numpy as np\n\ndef update_edge_distance(edge_distance, local_opt_tour, edge_n_used):\n    num_nodes = edge_distance.shape[0]\n    updated_edge_distance = edge_distance.copy()\n    \n    # Parameter initialization\n    topology_penalty_factor = 1.5\n    stochastic_scale = 0.8\n    normalization_weight = 0.6\n    \n    # Compute topology-aware penalties based on local optimal tour\n    for i in range(len(local_opt_tour) - 1):\n        u, v = local_opt_tour[i], local_opt_tour[i + 1]\n        penalty = topology_penalty_factor * (1 + np.exp(-edge_n_used[u, v] / (np.mean(edge_n_used) + 1e-8)))\n        updated_edge_distance[u, v] += penalty\n        updated_edge_distance[v, u] = updated_edge_distance[u, v]\n    \n    # Apply stochastic perturbations with adaptive scaling\n    stochastic_perturbation = stochastic_scale * np.random.uniform(0.5, 1.5, size=(num_nodes, num_nodes))\n    stochastic_perturbation = np.triu(stochastic_perturbation) + np.triu(stochastic_perturbation, 1).T\n    updated_edge_distance *= stochastic_perturbation\n    \n    # Introduce edge frequency normalization\n    normalized_frequencies = normalization_weight * (edge_n_used / (np.max(edge_n_used) + 1e-8))\n    updated_edge_distance -= normalized_frequencies\n    \n    # Ensure no negative distances\n    updated_edge_distance = np.maximum(updated_edge_distance, 0)\n    \n    return updated_edge_distance",
          "objective": 0.23271,
          "other_inf": null,
          "metadata": {
               "operator": "m1",
               "wisdom_tips": [
                    "Introduce topology-aware penalties to dynamically reshape the search landscape, ensuring decisions reflect both local structure and global connectivity.",
                    "Incorporate stochastic perturbations with adaptive scaling to balance exploration and exploitation, enhancing robustness across diverse problem instances.",
                    "Integrate dynamic penalties and adaptive scaling to reshape the search landscape iteratively, enhancing both exploration and exploitation."
               ],
               "attention_focus": "experimenting with hybrid strategy combinations",
               "generation_tendency": "focus_exploration",
               "timestamp": 1757181137.073193
          }
     },
     {
          "algorithm": "Design a hybrid algorithm that integrates dynamic asymmetry penalties, oscillatory perturbations, and adaptive edge reinforcement to iteratively reshape the search landscape while balancing exploration and exploitation through non-linear feedback mechanisms.",
          "code": "import numpy as np\n\ndef update_edge_distance(edge_distance, local_opt_tour, edge_n_used):\n    num_nodes = edge_distance.shape[0]\n    updated_edge_distance = edge_distance.copy()\n    \n    # Parameter initialization\n    asymmetry_penalty_factor = 1.2\n    oscillation_amplitude = 0.8\n    reinforcement_weight = 0.4\n    feedback_smoothing = 0.2\n    \n    # Compute dynamic asymmetry penalties based on edge usage imbalance\n    for i in range(len(local_opt_tour) - 1):\n        u, v = local_opt_tour[i], local_opt_tour[i + 1]\n        imbalance = np.abs(edge_n_used[u, v] - edge_n_used[v, u]) / (edge_n_used[u, v] + edge_n_used[v, u] + 1e-8)\n        penalty = asymmetry_penalty_factor * (1 + np.exp(-imbalance))\n        updated_edge_distance[u, v] += penalty\n        updated_edge_distance[v, u] += penalty\n    \n    # Introduce oscillatory perturbations with adaptive scaling\n    oscillation_scale = oscillation_amplitude * (1 + np.sin(np.pi * edge_n_used / (np.max(edge_n_used) + 1e-8)))\n    oscillatory_perturbation = oscillation_scale * (np.random.rand(num_nodes, num_nodes) - 0.5)\n    updated_edge_distance += oscillatory_perturbation\n    \n    # Apply adaptive edge reinforcement based on cumulative feedback\n    feedback_signal = reinforcement_weight * (edge_n_used - np.mean(edge_n_used)) / (np.std(edge_n_used) + 1e-8)\n    feedback_signal = np.tanh(feedback_signal * feedback_smoothing)  # Non-linear feedback smoothing\n    updated_edge_distance -= feedback_signal  # Reinforce by reducing distances\n    \n    # Ensure no negative distances\n    updated_edge_distance = np.maximum(updated_edge_distance, 0)\n    \n    return updated_edge_distance",
          "objective": 0.2474,
          "other_inf": null,
          "metadata": {
               "operator": "e1",
               "wisdom_tips": [
                    "Leverage entropy-driven strategies and adaptive scaling to maintain solution diversity and enhance the robustness of the optimization process.",
                    "Integrate topology-aware adjustments to dynamically reshape the search landscape, enhancing both exploration and exploitation in response to structural problem features.",
                    "Combine stochastic perturbations with cumulative feedback mechanisms to maintain solution diversity while iteratively improving convergence efficiency."
               ],
               "attention_focus": "experimenting with hybrid strategy combinations",
               "generation_tendency": "focus_exploration",
               "timestamp": 1757182006.133596
          }
     }
]