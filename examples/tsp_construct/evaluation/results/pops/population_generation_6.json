[
     {
          "algorithm": "The algorithm selects the next node by adaptively weighting nearest neighbors, edge cost normalization, and global coherence using a hybrid beam search with Monte Carlo rollouts, emphasizing robustness and dynamic adaptability.}\n\n```python\nimport numpy as np\n\ndef select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):\n    def monte_carlo_rollout(node, candidates, simulations=30):\n        scores = {candidate: 0 for candidate in candidates}\n        for _ in range(simulations):\n            for candidate in candidates:\n                remaining_nodes = [n for n in unvisited_nodes if n != candidate]\n                path = [current_node, candidate]\n                total_cost = distance_matrix[current_node][candidate]\n                while remaining_nodes:\n                    next_node = min(remaining_nodes, key=lambda x: distance_matrix[path[-1]][x])\n                    path.append(next_node)\n                    total_cost += distance_matrix[path[-2]][path[-1]]\n                    remaining_nodes.remove(next_node)\n                total_cost += distance_matrix[path[-1]][destination_node]\n                scores[candidate] += total_cost\n        return {k: v / simulations for k, v in scores.items()}\n    \n    def beam_search_candidates(candidates, scores, beam_width=5):\n        sorted_candidates = sorted(candidates, key=lambda x: scores[x])\n        return sorted_candidates[:beam_width]\n    \n    # Step 1: Generate a limited candidate set based on nearest neighbors\n    all_candidates = unvisited_nodes\n    candidate_set_size = min(15, len(all_candidates))  # Adjusted parameter\n    candidates = sorted(all_candidates, key=lambda x: distance_matrix[current_node][x])[:candidate_set_size]\n    \n    # Step 2: Perform Monte Carlo-like rollouts to evaluate candidates\n    rollout_scores = monte_carlo_rollout(current_node, candidates)\n    \n    # Step 3: Use beam search to refine candidate selection\n    refined_candidates = beam_search_candidates(candidates, rollout_scores)\n    \n    # Step 4: Dynamically weight local edge costs with global coherence and edge cost normalization\n    weighted_scores = {\n        candidate: (0.6 * distance_matrix[current_node][candidate] + 0.4 * rollout_scores[candidate]) / \n                   (1 + 0.25 * np.std([distance_matrix[current_node][x] for x in unvisited_nodes]))\n        for candidate in refined_candidates\n    ",
          "code": "import numpy as np\n\ndef select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):\n    def monte_carlo_rollout(node, candidates, simulations=30):\n        scores = {candidate: 0 for candidate in candidates}\n        for _ in range(simulations):\n            for candidate in candidates:\n                remaining_nodes = [n for n in unvisited_nodes if n != candidate]\n                path = [current_node, candidate]\n                total_cost = distance_matrix[current_node][candidate]\n                while remaining_nodes:\n                    next_node = min(remaining_nodes, key=lambda x: distance_matrix[path[-1]][x])\n                    path.append(next_node)\n                    total_cost += distance_matrix[path[-2]][path[-1]]\n                    remaining_nodes.remove(next_node)\n                total_cost += distance_matrix[path[-1]][destination_node]\n                scores[candidate] += total_cost\n        return {k: v / simulations for k, v in scores.items()}\n    \n    def beam_search_candidates(candidates, scores, beam_width=5):\n        sorted_candidates = sorted(candidates, key=lambda x: scores[x])\n        return sorted_candidates[:beam_width]\n    \n    # Step 1: Generate a limited candidate set based on nearest neighbors\n    all_candidates = unvisited_nodes\n    candidate_set_size = min(15, len(all_candidates))  # Adjusted parameter\n    candidates = sorted(all_candidates, key=lambda x: distance_matrix[current_node][x])[:candidate_set_size]\n    \n    # Step 2: Perform Monte Carlo-like rollouts to evaluate candidates\n    rollout_scores = monte_carlo_rollout(current_node, candidates)\n    \n    # Step 3: Use beam search to refine candidate selection\n    refined_candidates = beam_search_candidates(candidates, rollout_scores)\n    \n    # Step 4: Dynamically weight local edge costs with global coherence and edge cost normalization\n    weighted_scores = {\n        candidate: (0.6 * distance_matrix[current_node][candidate] + 0.4 * rollout_scores[candidate]) / \n                   (1 + 0.25 * np.std([distance_matrix[current_node][x] for x in unvisited_nodes]))\n        for candidate in refined_candidates\n    }\n    \n    # Step 5: Select the next node with the lowest weighted score\n    next_node = min(weighted_scores, key=weighted_scores.get)\n    \n    return next_node",
          "objective": 5.96053,
          "other_inf": null,
          "metadata": {
               "operator": "m2",
               "wisdom_tips": [
                    "Balance local optimization with global solution structure when making decisions",
                    "Use dynamic weighting mechanisms to adapt algorithm behavior based on current state",
                    "Consider the impact of current choices on future decision flexibility"
               ],
               "attention_focus": "improving algorithm robustness across different problem instances",
               "generation_tendency": "balanced_search",
               "timestamp": 1748280699.407766
          }
     },
     {
          "algorithm": "The algorithm selects the next node by balancing nearest-neighbor proximity, global path coherence, and future flexibility using adaptive weighting and cluster-aware Monte Carlo rollouts.}\n\n```python\nimport numpy as np\n\ndef select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):\n    def monte_carlo_rollout(node, candidates, simulations=10):\n        scores = {candidate: 0 for candidate in candidates}\n        for _ in range(simulations):\n            for candidate in candidates:\n                remaining_nodes = [n for n in unvisited_nodes if n != candidate]\n                path = [current_node, candidate]\n                total_cost = distance_matrix[current_node][candidate]\n                while remaining_nodes:\n                    next_node = min(remaining_nodes, key=lambda x: distance_matrix[path[-1]][x])\n                    path.append(next_node)\n                    total_cost += distance_matrix[path[-2]][path[-1]]\n                    remaining_nodes.remove(next_node)\n                total_cost += distance_matrix[path[-1]][destination_node]\n                scores[candidate] += total_cost\n        return {k: v / simulations for k, v in scores.items()}\n    \n    def beam_search_candidates(candidates, scores, beam_width=4):\n        sorted_candidates = sorted(candidates, key=lambda x: scores[x])\n        return sorted_candidates[:beam_width]\n    \n    def cluster_aware_weight(candidate, cluster_centers, weight_factor=0.7):\n        distances_to_clusters = [distance_matrix[candidate][center] for center in cluster_centers]\n        min_cluster_distance = min(distances_to_clusters) if distances_to_clusters else 0\n        return weight_factor * min_cluster_distance\n    \n    # Step 1: Generate a limited candidate set based on nearest neighbors and clusters\n    all_candidates = unvisited_nodes\n    candidate_set_size = min(5, len(all_candidates))  # Adjusted parameter\n    candidates = sorted(all_candidates, key=lambda x: distance_matrix[current_node][x])[:candidate_set_size]\n    cluster_centers = [node for node in unvisited_nodes if distance_matrix[current_node][node] < np.mean(distance_matrix[current_node])]\n    \n    # Step 2: Perform Monte Carlo-like rollouts to evaluate candidates\n    rollout_scores = monte_carlo_rollout(current_node, candidates)\n    \n    # Step 3: Use beam search to refine candidate selection\n    refined_candidates = beam_search_candidates(candidates, rollout_scores)\n    \n    # Step 4: Dynamically weight local edge costs, global coherence, and cluster awareness\n    weighted_scores = {\n        candidate: (\n            0.4 * distance_matrix[current_node][candidate] + \n            0.4 * rollout_scores[candidate] + \n            0.2 * cluster_aware_weight(candidate, cluster_centers)\n        )\n        for candidate in refined_candidates\n    ",
          "code": "import numpy as np\n\ndef select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):\n    def monte_carlo_rollout(node, candidates, simulations=10):\n        scores = {candidate: 0 for candidate in candidates}\n        for _ in range(simulations):\n            for candidate in candidates:\n                remaining_nodes = [n for n in unvisited_nodes if n != candidate]\n                path = [current_node, candidate]\n                total_cost = distance_matrix[current_node][candidate]\n                while remaining_nodes:\n                    next_node = min(remaining_nodes, key=lambda x: distance_matrix[path[-1]][x])\n                    path.append(next_node)\n                    total_cost += distance_matrix[path[-2]][path[-1]]\n                    remaining_nodes.remove(next_node)\n                total_cost += distance_matrix[path[-1]][destination_node]\n                scores[candidate] += total_cost\n        return {k: v / simulations for k, v in scores.items()}\n    \n    def beam_search_candidates(candidates, scores, beam_width=4):\n        sorted_candidates = sorted(candidates, key=lambda x: scores[x])\n        return sorted_candidates[:beam_width]\n    \n    def cluster_aware_weight(candidate, cluster_centers, weight_factor=0.7):\n        distances_to_clusters = [distance_matrix[candidate][center] for center in cluster_centers]\n        min_cluster_distance = min(distances_to_clusters) if distances_to_clusters else 0\n        return weight_factor * min_cluster_distance\n    \n    # Step 1: Generate a limited candidate set based on nearest neighbors and clusters\n    all_candidates = unvisited_nodes\n    candidate_set_size = min(5, len(all_candidates))  # Adjusted parameter\n    candidates = sorted(all_candidates, key=lambda x: distance_matrix[current_node][x])[:candidate_set_size]\n    cluster_centers = [node for node in unvisited_nodes if distance_matrix[current_node][node] < np.mean(distance_matrix[current_node])]\n    \n    # Step 2: Perform Monte Carlo-like rollouts to evaluate candidates\n    rollout_scores = monte_carlo_rollout(current_node, candidates)\n    \n    # Step 3: Use beam search to refine candidate selection\n    refined_candidates = beam_search_candidates(candidates, rollout_scores)\n    \n    # Step 4: Dynamically weight local edge costs, global coherence, and cluster awareness\n    weighted_scores = {\n        candidate: (\n            0.4 * distance_matrix[current_node][candidate] + \n            0.4 * rollout_scores[candidate] + \n            0.2 * cluster_aware_weight(candidate, cluster_centers)\n        )\n        for candidate in refined_candidates\n    }\n    \n    # Step 5: Select the next node with the lowest weighted score\n    next_node = min(weighted_scores, key=weighted_scores.get)\n    \n    return next_node",
          "objective": 5.96185,
          "other_inf": null,
          "metadata": {
               "operator": "m2",
               "wisdom_tips": [
                    "Balance local optimization with global solution structure when making decisions",
                    "Use dynamic weighting mechanisms to adapt algorithm behavior based on current state",
                    "Consider the impact of current choices on future decision flexibility"
               ],
               "attention_focus": "refining core evaluation and scoring functions",
               "generation_tendency": "focus_exploitation",
               "timestamp": 1748279584.304579
          }
     },
     {
          "algorithm": "The algorithm dynamically balances nearest neighbor selection, global coherence via edge cost normalization, and future flexibility using Monte Carlo-like rollouts with adjusted weights and beam search refinement.}\n\n```python\nimport numpy as np\n\ndef select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):\n    def monte_carlo_rollout(node, candidates, simulations=20):\n        scores = {candidate: 0 for candidate in candidates}\n        for _ in range(simulations):\n            for candidate in candidates:\n                remaining_nodes = [n for n in unvisited_nodes if n != candidate]\n                path = [current_node, candidate]\n                total_cost = distance_matrix[current_node][candidate]\n                while remaining_nodes:\n                    next_node = min(remaining_nodes, key=lambda x: distance_matrix[path[-1]][x])\n                    path.append(next_node)\n                    total_cost += distance_matrix[path[-2]][path[-1]]\n                    remaining_nodes.remove(next_node)\n                total_cost += distance_matrix[path[-1]][destination_node]\n                scores[candidate] += total_cost\n        return {k: v / simulations for k, v in scores.items()}\n    \n    def beam_search_candidates(candidates, scores, beam_width=8):\n        sorted_candidates = sorted(candidates, key=lambda x: scores[x])\n        return sorted_candidates[:beam_width]\n    \n    # Step 1: Generate a limited candidate set based on nearest neighbors\n    all_candidates = unvisited_nodes\n    candidate_set_size = min(10, len(all_candidates))  # Adjusted parameter\n    candidates = sorted(all_candidates, key=lambda x: distance_matrix[current_node][x])[:candidate_set_size]\n    \n    # Step 2: Perform Monte Carlo-like rollouts to evaluate candidates\n    rollout_scores = monte_carlo_rollout(current_node, candidates)\n    \n    # Step 3: Use beam search to refine candidate selection\n    refined_candidates = beam_search_candidates(candidates, rollout_scores)\n    \n    # Step 4: Dynamically weight local edge costs with global coherence and edge cost normalization\n    weighted_scores = {\n        candidate: (0.6 * distance_matrix[current_node][candidate] + 0.4 * rollout_scores[candidate]) / \n                   (1 + 0.15 * np.std([distance_matrix[current_node][x] for x in unvisited_nodes]))\n        for candidate in refined_candidates\n    ",
          "code": "import numpy as np\n\ndef select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):\n    def monte_carlo_rollout(node, candidates, simulations=20):\n        scores = {candidate: 0 for candidate in candidates}\n        for _ in range(simulations):\n            for candidate in candidates:\n                remaining_nodes = [n for n in unvisited_nodes if n != candidate]\n                path = [current_node, candidate]\n                total_cost = distance_matrix[current_node][candidate]\n                while remaining_nodes:\n                    next_node = min(remaining_nodes, key=lambda x: distance_matrix[path[-1]][x])\n                    path.append(next_node)\n                    total_cost += distance_matrix[path[-2]][path[-1]]\n                    remaining_nodes.remove(next_node)\n                total_cost += distance_matrix[path[-1]][destination_node]\n                scores[candidate] += total_cost\n        return {k: v / simulations for k, v in scores.items()}\n    \n    def beam_search_candidates(candidates, scores, beam_width=8):\n        sorted_candidates = sorted(candidates, key=lambda x: scores[x])\n        return sorted_candidates[:beam_width]\n    \n    # Step 1: Generate a limited candidate set based on nearest neighbors\n    all_candidates = unvisited_nodes\n    candidate_set_size = min(10, len(all_candidates))  # Adjusted parameter\n    candidates = sorted(all_candidates, key=lambda x: distance_matrix[current_node][x])[:candidate_set_size]\n    \n    # Step 2: Perform Monte Carlo-like rollouts to evaluate candidates\n    rollout_scores = monte_carlo_rollout(current_node, candidates)\n    \n    # Step 3: Use beam search to refine candidate selection\n    refined_candidates = beam_search_candidates(candidates, rollout_scores)\n    \n    # Step 4: Dynamically weight local edge costs with global coherence and edge cost normalization\n    weighted_scores = {\n        candidate: (0.6 * distance_matrix[current_node][candidate] + 0.4 * rollout_scores[candidate]) / \n                   (1 + 0.15 * np.std([distance_matrix[current_node][x] for x in unvisited_nodes]))\n        for candidate in refined_candidates\n    }\n    \n    # Step 5: Select the next node with the lowest weighted score\n    next_node = min(weighted_scores, key=weighted_scores.get)\n    \n    return next_node",
          "objective": 5.96866,
          "other_inf": null,
          "metadata": {
               "operator": "m2",
               "wisdom_tips": [
                    "Balance local optimization with global solution structure when making decisions",
                    "Use dynamic weighting mechanisms to adapt algorithm behavior based on current state",
                    "Consider the impact of current choices on future decision flexibility"
               ],
               "attention_focus": "optimizing established successful strategies and patterns",
               "generation_tendency": "focus_exploitation",
               "timestamp": 1748280104.4261801
          }
     },
     {
          "algorithm": "The algorithm selects the next node by dynamically weighting nearest neighbors, global path coherence, and edge cost normalization using an adaptive beam search mechanism with Monte Carlo-like rollouts, emphasizing future flexibility and local-global balance.}\n\n```python\nimport numpy as np\n\ndef select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):\n    def monte_carlo_rollout(node, candidates, simulations=20):\n        scores = {candidate: 0 for candidate in candidates}\n        for _ in range(simulations):\n            for candidate in candidates:\n                remaining_nodes = [n for n in unvisited_nodes if n != candidate]\n                path = [current_node, candidate]\n                total_cost = distance_matrix[current_node][candidate]\n                while remaining_nodes:\n                    next_node = min(remaining_nodes, key=lambda x: distance_matrix[path[-1]][x])\n                    path.append(next_node)\n                    total_cost += distance_matrix[path[-2]][path[-1]]\n                    remaining_nodes.remove(next_node)\n                total_cost += distance_matrix[path[-1]][destination_node]\n                scores[candidate] += total_cost\n        return {k: v / simulations for k, v in scores.items()}\n    \n    def beam_search_candidates(candidates, scores, beam_width=8):\n        sorted_candidates = sorted(candidates, key=lambda x: scores[x])\n        return sorted_candidates[:beam_width]\n    \n    # Step 1: Generate a limited candidate set based on nearest neighbors\n    all_candidates = unvisited_nodes\n    candidate_set_size = min(10, len(all_candidates))  # Adjusted parameter\n    candidates = sorted(all_candidates, key=lambda x: distance_matrix[current_node][x])[:candidate_set_size]\n    \n    # Step 2: Perform Monte Carlo-like rollouts to evaluate candidates\n    rollout_scores = monte_carlo_rollout(current_node, candidates)\n    \n    # Step 3: Use beam search to refine candidate selection\n    refined_candidates = beam_search_candidates(candidates, rollout_scores)\n    \n    # Step 4: Dynamically weight local edge costs with global coherence and edge cost normalization\n    weighted_scores = {\n        candidate: (0.4 * distance_matrix[current_node][candidate] + 0.6 * rollout_scores[candidate]) / \n                   (1 + 0.15 * np.std([distance_matrix[current_node][x] for x in unvisited_nodes]))\n        for candidate in refined_candidates\n    ",
          "code": "import numpy as np\n\ndef select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):\n    def monte_carlo_rollout(node, candidates, simulations=20):\n        scores = {candidate: 0 for candidate in candidates}\n        for _ in range(simulations):\n            for candidate in candidates:\n                remaining_nodes = [n for n in unvisited_nodes if n != candidate]\n                path = [current_node, candidate]\n                total_cost = distance_matrix[current_node][candidate]\n                while remaining_nodes:\n                    next_node = min(remaining_nodes, key=lambda x: distance_matrix[path[-1]][x])\n                    path.append(next_node)\n                    total_cost += distance_matrix[path[-2]][path[-1]]\n                    remaining_nodes.remove(next_node)\n                total_cost += distance_matrix[path[-1]][destination_node]\n                scores[candidate] += total_cost\n        return {k: v / simulations for k, v in scores.items()}\n    \n    def beam_search_candidates(candidates, scores, beam_width=8):\n        sorted_candidates = sorted(candidates, key=lambda x: scores[x])\n        return sorted_candidates[:beam_width]\n    \n    # Step 1: Generate a limited candidate set based on nearest neighbors\n    all_candidates = unvisited_nodes\n    candidate_set_size = min(10, len(all_candidates))  # Adjusted parameter\n    candidates = sorted(all_candidates, key=lambda x: distance_matrix[current_node][x])[:candidate_set_size]\n    \n    # Step 2: Perform Monte Carlo-like rollouts to evaluate candidates\n    rollout_scores = monte_carlo_rollout(current_node, candidates)\n    \n    # Step 3: Use beam search to refine candidate selection\n    refined_candidates = beam_search_candidates(candidates, rollout_scores)\n    \n    # Step 4: Dynamically weight local edge costs with global coherence and edge cost normalization\n    weighted_scores = {\n        candidate: (0.4 * distance_matrix[current_node][candidate] + 0.6 * rollout_scores[candidate]) / \n                   (1 + 0.15 * np.std([distance_matrix[current_node][x] for x in unvisited_nodes]))\n        for candidate in refined_candidates\n    }\n    \n    # Step 5: Select the next node with the lowest weighted score\n    next_node = min(weighted_scores, key=weighted_scores.get)\n    \n    return next_node",
          "objective": 5.96986,
          "other_inf": null,
          "metadata": {
               "operator": "m2",
               "wisdom_tips": [
                    "Balance local optimization with global solution structure when making decisions",
                    "Use dynamic weighting mechanisms to adapt algorithm behavior based on current state",
                    "Consider the impact of current choices on future decision flexibility"
               ],
               "attention_focus": "improving precision of existing heuristics and rules",
               "generation_tendency": "focus_exploitation",
               "timestamp": 1748279584.308521
          }
     }
]