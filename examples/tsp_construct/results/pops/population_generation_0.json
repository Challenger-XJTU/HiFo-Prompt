[
     {
          "algorithm": "The algorithm iteratively selects the next node by maintaining a limited candidate set for each node, using look-ahead evaluation to assess future moves, leveraging minimum spanning tree properties and edge cost normalization, considering path momentum and direction, identifying nearest neighbor clusters, employing MCTS-inspired lightweight simulations, and guiding exploration with a beam search-like mechanism to balance local optimality with global search strategies.}\n\n```python\nimport numpy as np\n\ndef select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):\n    def calculate_mst_cost(nodes):\n        if len(nodes) <= 1:\n            return 0\n        edges = [(distance_matrix[i][j], i, j) for i in nodes for j in nodes if i < j]\n        edges.sort(key=lambda x: x[0])\n        parent = {node: node for node in nodes}\n        rank = {node: 0 for node in nodes",
          "code": "import numpy as np\n\ndef select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):\n    def calculate_mst_cost(nodes):\n        if len(nodes) <= 1:\n            return 0\n        edges = [(distance_matrix[i][j], i, j) for i in nodes for j in nodes if i < j]\n        edges.sort(key=lambda x: x[0])\n        parent = {node: node for node in nodes}\n        rank = {node: 0 for node in nodes}\n        \n        def find(node):\n            while parent[node] != node:\n                parent[node] = parent[parent[node]]\n                node = parent[node]\n            return node\n        \n        def union(node1, node2):\n            root1 = find(node1)\n            root2 = find(node2)\n            if root1 != root2:\n                if rank[root1] > rank[root2]:\n                    parent[root2] = root1\n                else:\n                    parent[root1] = root2\n                    if rank[root1] == rank[root2]:\n                        rank[root2] += 1\n        \n        mst_cost = 0\n        for cost, u, v in edges:\n            if find(u) != find(v):\n                union(u, v)\n                mst_cost += cost\n        return mst_cost\n    \n    def simulate_path(start, remaining, depth=3):\n        if depth == 0 or not remaining:\n            return 0\n        best_cost = float('inf')\n        for next_node in remaining:\n            new_remaining = [n for n in remaining if n != next_node]\n            cost = distance_matrix[start][next_node] + simulate_path(next_node, new_remaining, depth - 1)\n            best_cost = min(best_cost, cost)\n        return best_cost\n    \n    candidates = sorted(unvisited_nodes, key=lambda x: distance_matrix[current_node][x])[:min(5, len(unvisited_nodes))]\n    scores = []\n    \n    for candidate in candidates:\n        look_ahead_cost = simulate_path(candidate, [n for n in unvisited_nodes if n != candidate])\n        mst_cost = calculate_mst_cost([candidate] + [n for n in unvisited_nodes if n != candidate] + [destination_node])\n        normalized_cost = distance_matrix[current_node][candidate] / (np.mean(distance_matrix[current_node]) + 1e-6)\n        momentum_direction = np.dot(\n            np.array([distance_matrix[current_node][candidate], distance_matrix[candidate][destination_node]]),\n            np.array([1, -1])\n        )\n        cluster_score = len([n for n in unvisited_nodes if distance_matrix[candidate][n] < np.percentile(distance_matrix[candidate], 25)])\n        score = (look_ahead_cost + mst_cost) * normalized_cost - momentum_direction + cluster_score\n        scores.append((score, candidate))\n    \n    _, next_node = min(scores, key=lambda x: x[0])\n    return next_node",
          "objective": 7.4247,
          "other_inf": null,
          "metadata": {
               "operator": "i1",
               "insights": [
                    "Design adaptive hybrid meta-heuristics synergistically fusing multiple search paradigms and dynamically tune operator parameters based on search stage or problem features",
                    "Employ machine learning or pattern recognition to mine deep problem structures and optimal solution patterns then use learned insights to intelligently bias towards promising search regions or constructive choices",
                    "Explore objective function engineering by introducing auxiliary or surrogate objectives or by dynamically adjusting weights to reshape the search landscape aiding escape from local optima or guiding diverse exploration"
               ],
               "attention_focus": "balancing local optimality with global search strategies",
               "generation_tendency": "balanced_search",
               "timestamp": 1769746640.7725508
          }
     },
     {
          "algorithm": "The algorithm iteratively selects the next node by integrating adaptive candidate sets, look-ahead evaluation via lightweight Monte Carlo simulations, MST-based edge cost normalization, path momentum analysis, nearest neighbor clustering, and beam search-guided exploration to balance local refinement and global exploration.",
          "code": "import numpy as np\n\ndef select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):\n    def mst_based_cost(edge_cost, global_min, global_max):\n        return (edge_cost - global_min) / (global_max - global_min + 1e-10)\n    \n    def simulate_path(node, remaining, depth=3):\n        if depth == 0 or len(remaining) == 0:\n            return 0\n        candidates = sorted(remaining, key=lambda x: distance_matrix[node, x])[:5]\n        return min(distance_matrix[node, c] + simulate_path(c, [n for n in remaining if n != c], depth-1) for c in candidates)\n    \n    def cluster_score(node, unvisited):\n        distances = [distance_matrix[node, n] for n in unvisited]\n        return np.mean(distances) if distances else float('inf')\n    \n    global_min = np.min(distance_matrix[np.nonzero(distance_matrix)])\n    global_max = np.max(distance_matrix)\n    beam_width = 5\n    look_ahead_depth = 3\n    \n    # Adaptive candidate set\n    candidate_set = sorted(unvisited_nodes, key=lambda x: distance_matrix[current_node, x])[:beam_width]\n    \n    # Evaluate candidates using look-ahead, MST cost normalization, and clustering\n    scores = []\n    for candidate in candidate_set:\n        look_ahead_cost = simulate_path(candidate, [n for n in unvisited_nodes if n != candidate], look_ahead_depth)\n        normalized_cost = mst_based_cost(distance_matrix[current_node, candidate], global_min, global_max)\n        cluster_quality = cluster_score(candidate, unvisited_nodes)\n        momentum_penalty = np.linalg.norm(np.array([distance_matrix[current_node, candidate]]) - \n                                          np.array([distance_matrix[destination_node, candidate]]))\n        score = look_ahead_cost + normalized_cost + cluster_quality + momentum_penalty\n        scores.append((candidate, score))\n    \n    # Beam search-like selection\n    best_candidates = sorted(scores, key=lambda x: x[1])[:beam_width]\n    next_node = min(best_candidates, key=lambda x: x[1])[0]\n    \n    return next_node",
          "objective": 13.02481,
          "other_inf": null,
          "metadata": {
               "operator": "i1",
               "insights": [
                    "Design adaptive hybrid meta-heuristics synergistically fusing multiple search paradigms and dynamically tune operator parameters based on search stage or problem features",
                    "Employ machine learning or pattern recognition to mine deep problem structures and optimal solution patterns then use learned insights to intelligently bias towards promising search regions or constructive choices",
                    "Explore objective function engineering by introducing auxiliary or surrogate objectives or by dynamically adjusting weights to reshape the search landscape aiding escape from local optima or guiding diverse exploration"
               ],
               "attention_focus": "improving algorithm robustness across different problem instances",
               "generation_tendency": "balanced_search",
               "timestamp": 1769746622.34991
          }
     },
     {
          "algorithm": "The algorithm integrates adaptive candidate set filtering, look-ahead evaluation via lightweight Monte Carlo simulations, MST-based edge cost normalization, path momentum analysis, nearest neighbor clustering, and beam search-guided exploration to robustly select the next node.}\n\n```python\nimport numpy as np\n\ndef select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):\n    def calculate_mst_cost(nodes):\n        if len(nodes) <= 1:\n            return 0\n        edges = [(distance_matrix[i][j], i, j) for i in nodes for j in nodes if i < j]\n        edges.sort(key=lambda x: x[0])\n        parent = {node: node for node in nodes}\n        rank = {node: 0 for node in nodes}\n        \n        def find(u):\n            while parent[u] != u:\n                parent[u] = parent[parent[u]]\n                u = parent[u]\n            return u\n        \n        def union(u, v):\n            root_u = find(u)\n            root_v = find(v)\n            if root_u == root_v:\n                return False\n            if rank[root_u] > rank[root_v]:\n                parent[root_v] = root_u\n            elif rank[root_u] < rank[root_v]:\n                parent[root_u] = root_v\n            else:\n                parent[root_v] = root_u\n                rank[root_u] += 1\n            return True\n        \n        mst_cost = 0\n        for cost, u, v in edges:\n            if union(u, v):\n                mst_cost += cost\n        return mst_cost\n    \n    def simulate_path(start, remaining, steps=3):\n        if steps == 0 or not remaining:\n            return 0\n        candidates = sorted(remaining, key=lambda x: distance_matrix[start][x])[:5]\n        return min(distance_matrix[start][cand] + simulate_path(cand, [n for n in remaining if n != cand], steps-1) for cand in candidates)\n    \n    # Candidate set filtering\n    candidates = sorted(unvisited_nodes, key=lambda x: distance_matrix[current_node][x])[:10]\n    \n    # Look-ahead evaluation using lightweight Monte Carlo simulation\n    lookahead_scores = {node: simulate_path(node, [n for n in unvisited_nodes if n != node]) for node in candidates}\n    \n    # MST-based edge cost normalization\n    all_nodes = list(unvisited_nodes) + [current_node, destination_node]\n    mst_cost = calculate_mst_cost(all_nodes)\n    normalized_costs = {node: distance_matrix[current_node][node] / (mst_cost + 1e-6) for node in candidates}\n    \n    # Path momentum and direction\n    momentum_score = {node: np.dot(np.array([distance_matrix[current_node][node], 0]), \n                                   np.array([distance_matrix[node][destination_node], 0])) for node in candidates}\n    \n    # Nearest neighbor clustering\n    cluster_score = {node: sum(distance_matrix[node][n] for n in candidates if n != node) for node in candidates}\n    \n    # Combine scores\n    combined_scores = {node: (lookahead_scores[node] * 0.4 + \n                              normalized_costs[node] * 0.2 + \n                              momentum_score[node] * 0.2 + \n                              cluster_score[node] * 0.2) for node in candidates",
          "code": "import numpy as np\n\ndef select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):\n    def calculate_mst_cost(nodes):\n        if len(nodes) <= 1:\n            return 0\n        edges = [(distance_matrix[i][j], i, j) for i in nodes for j in nodes if i < j]\n        edges.sort(key=lambda x: x[0])\n        parent = {node: node for node in nodes}\n        rank = {node: 0 for node in nodes}\n        \n        def find(u):\n            while parent[u] != u:\n                parent[u] = parent[parent[u]]\n                u = parent[u]\n            return u\n        \n        def union(u, v):\n            root_u = find(u)\n            root_v = find(v)\n            if root_u == root_v:\n                return False\n            if rank[root_u] > rank[root_v]:\n                parent[root_v] = root_u\n            elif rank[root_u] < rank[root_v]:\n                parent[root_u] = root_v\n            else:\n                parent[root_v] = root_u\n                rank[root_u] += 1\n            return True\n        \n        mst_cost = 0\n        for cost, u, v in edges:\n            if union(u, v):\n                mst_cost += cost\n        return mst_cost\n    \n    def simulate_path(start, remaining, steps=3):\n        if steps == 0 or not remaining:\n            return 0\n        candidates = sorted(remaining, key=lambda x: distance_matrix[start][x])[:5]\n        return min(distance_matrix[start][cand] + simulate_path(cand, [n for n in remaining if n != cand], steps-1) for cand in candidates)\n    \n    # Candidate set filtering\n    candidates = sorted(unvisited_nodes, key=lambda x: distance_matrix[current_node][x])[:10]\n    \n    # Look-ahead evaluation using lightweight Monte Carlo simulation\n    lookahead_scores = {node: simulate_path(node, [n for n in unvisited_nodes if n != node]) for node in candidates}\n    \n    # MST-based edge cost normalization\n    all_nodes = list(unvisited_nodes) + [current_node, destination_node]\n    mst_cost = calculate_mst_cost(all_nodes)\n    normalized_costs = {node: distance_matrix[current_node][node] / (mst_cost + 1e-6) for node in candidates}\n    \n    # Path momentum and direction\n    momentum_score = {node: np.dot(np.array([distance_matrix[current_node][node], 0]), \n                                   np.array([distance_matrix[node][destination_node], 0])) for node in candidates}\n    \n    # Nearest neighbor clustering\n    cluster_score = {node: sum(distance_matrix[node][n] for n in candidates if n != node) for node in candidates}\n    \n    # Combine scores\n    combined_scores = {node: (lookahead_scores[node] * 0.4 + \n                              normalized_costs[node] * 0.2 + \n                              momentum_score[node] * 0.2 + \n                              cluster_score[node] * 0.2) for node in candidates}\n    \n    # Beam search-inspired selection\n    top_candidates = sorted(combined_scores.keys(), key=lambda x: combined_scores[x])[:3]\n    \n    # Final selection based on combined scores\n    next_node = min(top_candidates, key=lambda x: combined_scores[x])\n    return next_node",
          "objective": 13.5348,
          "other_inf": null,
          "metadata": {
               "operator": "i1",
               "insights": [
                    "Design adaptive hybrid meta-heuristics synergistically fusing multiple search paradigms and dynamically tune operator parameters based on search stage or problem features",
                    "Employ machine learning or pattern recognition to mine deep problem structures and optimal solution patterns then use learned insights to intelligently bias towards promising search regions or constructive choices",
                    "Explore objective function engineering by introducing auxiliary or surrogate objectives or by dynamically adjusting weights to reshape the search landscape aiding escape from local optima or guiding diverse exploration"
               ],
               "attention_focus": "improving algorithm robustness across different problem instances",
               "generation_tendency": "balanced_search",
               "timestamp": 1769746640.055098
          }
     }
]