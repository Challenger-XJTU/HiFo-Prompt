[
     {
          "algorithm": "node: node for node in nodes",
          "code": "import numpy as np\n\ndef select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):\n    def mst_heuristic(nodes, dist_matrix):\n        if len(nodes) == 0:\n            return 0\n        edges = [(dist_matrix[i][j], i, j) for i in nodes for j in nodes if i < j]\n        edges.sort(key=lambda x: x[0])\n        parent = {node: node for node in nodes}\n        \n        def find(u):\n            while parent[u] != u:\n                parent[u] = parent[parent[u]]\n                u = parent[u]\n            return u\n        \n        mst_cost = 0\n        for cost, u, v in edges:\n            if find(u) != find(v):\n                parent[find(v)] = find(u)\n                mst_cost += cost\n        return mst_cost\n    \n    def reinforcement_score(node, remaining_nodes, dist_matrix):\n        future_cost = mst_heuristic(remaining_nodes, dist_matrix)\n        return distance_matrix[current_node][node] + future_cost\n    \n    # Adaptive candidate set generation\n    candidate_set_size = max(3, len(unvisited_nodes) // 2)\n    candidate_set = sorted(unvisited_nodes, key=lambda x: distance_matrix[current_node][x])[:candidate_set_size]\n    \n    # Hybrid scoring mechanism\n    scores = []\n    for node in candidate_set:\n        remaining_after_choice = [n for n in unvisited_nodes if n != node]\n        score = reinforcement_score(node, remaining_after_choice, distance_matrix)\n        scores.append((node, score))\n    \n    # Select the node with the lowest score\n    next_node = min(scores, key=lambda x: x[1])[0]\n    return next_node",
          "objective": 6.38352,
          "other_inf": null,
          "metadata": {
               "operator": "m3",
               "wisdom_tips": [
                    "Design adaptive hybrid meta-heuristics synergistically fusing multiple search paradigms and dynamically tune operator parameters based on search stage or problem features",
                    "Employ machine learning or pattern recognition to mine deep problem structures and optimal solution patterns then use learned insights to intelligently bias towards promising search regions or constructive choices",
                    "Explore objective function engineering by introducing auxiliary or surrogate objectives or by dynamically adjusting weights to reshape the search landscape aiding escape from local optima or guiding diverse exploration"
               ],
               "attention_focus": "managing computational complexity and time efficiency",
               "generation_tendency": "balanced_search",
               "timestamp": 1748448095.1695201
          }
     },
     {
          "algorithm": "The algorithm adaptively combines greedy selection, MST-based cost normalization, dynamic clustering, and reinforcement learning-inspired scoring to iteratively choose the next node while dynamically adjusting candidate sets and exploration-exploitation trade-offs.}\n\n```python\nimport numpy as np\nfrom sklearn.cluster import KMeans\n\ndef select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):\n    def mst_heuristic(nodes, dist_matrix):\n        if len(nodes) == 0:\n            return 0\n        edges = [(dist_matrix[i][j], i, j) for i in nodes for j in nodes if i != j]\n        edges.sort(key=lambda x: x[0])\n        parent = {node: node for node in nodes}\n        rank = {node: 0 for node in nodes",
          "code": "import numpy as np\nfrom sklearn.cluster import KMeans\n\ndef select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):\n    def mst_heuristic(nodes, dist_matrix):\n        if len(nodes) == 0:\n            return 0\n        edges = [(dist_matrix[i][j], i, j) for i in nodes for j in nodes if i != j]\n        edges.sort(key=lambda x: x[0])\n        parent = {node: node for node in nodes}\n        rank = {node: 0 for node in nodes}\n        \n        def find(u):\n            while parent[u] != u:\n                parent[u] = parent[parent[u]]\n                u = parent[u]\n            return u\n        \n        def union(u, v):\n            root_u = find(u)\n            root_v = find(v)\n            if root_u != root_v:\n                if rank[root_u] > rank[root_v]:\n                    parent[root_v] = root_u\n                elif rank[root_u] < rank[root_v]:\n                    parent[root_u] = root_v\n                else:\n                    parent[root_v] = root_u\n                    rank[root_u] += 1\n        \n        mst_cost = 0\n        for cost, u, v in edges:\n            if find(u) != find(v):\n                union(u, v)\n                mst_cost += cost\n        return mst_cost\n    \n    def dynamic_clustering(nodes, dist_matrix):\n        coords = np.array([[i, np.mean([dist_matrix[i][j] for j in nodes])] for i in nodes])\n        n_clusters = min(4, len(nodes))\n        kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(coords)\n        clusters = [list(np.array(nodes)[kmeans.labels_ == i]) for i in range(kmeans.n_clusters)]\n        return clusters\n    \n    def reinforcement_score(node, remaining_nodes, clusters, dist_matrix):\n        cluster_bonus = min([np.mean([dist_matrix[node][n] for n in cluster]) for cluster in clusters]) if clusters else 0\n        future_cost = mst_heuristic(remaining_nodes, dist_matrix)\n        return dist_matrix[current_node][node] + future_cost - cluster_bonus\n    \n    # Adaptive candidate set generation\n    candidate_set_size = max(3, len(unvisited_nodes) // 2)\n    candidate_set = sorted(unvisited_nodes, key=lambda x: distance_matrix[current_node][x])[:candidate_set_size]\n    \n    # Dynamic clustering of unvisited nodes\n    clusters = dynamic_clustering(unvisited_nodes, distance_matrix)\n    \n    # Hybrid scoring mechanism\n    scores = []\n    for node in candidate_set:\n        remaining_after_choice = [n for n in unvisited_nodes if n != node]\n        score = reinforcement_score(node, remaining_after_choice, clusters, distance_matrix)\n        scores.append((node, score))\n    \n    # Select the node with the lowest score\n    next_node = min(scores, key=lambda x: x[1])[0]\n    return next_node",
          "objective": 6.42059,
          "other_inf": null,
          "metadata": {
               "operator": "m1",
               "wisdom_tips": [
                    "Design adaptive hybrid meta-heuristics synergistically fusing multiple search paradigms and dynamically tune operator parameters based on search stage or problem features",
                    "Employ machine learning or pattern recognition to mine deep problem structures and optimal solution patterns then use learned insights to intelligently bias towards promising search regions or constructive choices",
                    "Explore objective function engineering by introducing auxiliary or surrogate objectives or by dynamically adjusting weights to reshape the search landscape aiding escape from local optima or guiding diverse exploration"
               ],
               "attention_focus": "experimenting with hybrid strategy combinations",
               "generation_tendency": "balanced_search",
               "timestamp": 1748447921.475094
          }
     },
     {
          "algorithm": "The algorithm selects the next node by iteratively evaluating candidate sets using look-ahead simulations, MST-inspired edge cost normalization, path momentum, and clustering insights, while leveraging lightweight Monte Carlo Tree Search reasoning and beam search-guided exploration to balance short-term gains with long-term tour quality.}\n\n```python\nimport numpy as np\n\ndef select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):\n    def mst_cost(nodes):\n        if len(nodes) <= 1:\n            return 0\n        edges = sorted([(distance_matrix[n1][n2], n1, n2) for i, n1 in enumerate(nodes) for n2 in nodes[i+1:]])\n        parent = {node: node for node in nodes}\n        rank = {node: 0 for node in nodes",
          "code": "import numpy as np\n\ndef select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):\n    def mst_cost(nodes):\n        if len(nodes) <= 1:\n            return 0\n        edges = sorted([(distance_matrix[n1][n2], n1, n2) for i, n1 in enumerate(nodes) for n2 in nodes[i+1:]])\n        parent = {node: node for node in nodes}\n        rank = {node: 0 for node in nodes}\n        \n        def find(node):\n            while parent[node] != node:\n                parent[node] = parent[parent[node]]\n                node = parent[node]\n            return node\n        \n        def union(n1, n2):\n            root1, root2 = find(n1), find(n2)\n            if root1 == root2:\n                return False\n            if rank[root1] > rank[root2]:\n                parent[root2] = root1\n            else:\n                parent[root1] = root2\n                if rank[root1] == rank[root2]:\n                    rank[root2] += 1\n            return True\n        \n        total_cost = 0\n        for cost, n1, n2 in edges:\n            if union(n1, n2):\n                total_cost += cost\n        return total_cost\n    \n    def simulate_path(start, remaining, steps=3):\n        if steps == 0 or len(remaining) == 0:\n            return 0\n        best_simulated_cost = float('inf')\n        for next_n in remaining:\n            new_remaining = [n for n in remaining if n != next_n]\n            simulated_cost = distance_matrix[start][next_n] + simulate_path(next_n, new_remaining, steps-1)\n            best_simulated_cost = min(best_simulated_cost, simulated_cost)\n        return best_simulated_cost\n    \n    candidates = unvisited_nodes[:min(5, len(unvisited_nodes))]  # Limit candidate set size\n    scores = []\n    \n    for candidate in candidates:\n        future_nodes = [n for n in unvisited_nodes if n != candidate]\n        look_ahead_cost = simulate_path(candidate, future_nodes)\n        mst_remaining_cost = mst_cost(future_nodes + [destination_node])\n        normalized_edge_cost = distance_matrix[current_node][candidate] / (np.mean(distance_matrix[current_node]) + 1e-6)\n        momentum_direction = np.dot(np.array([distance_matrix[current_node][candidate]]),\n                                    np.array([distance_matrix[candidate][destination_node]]))\n        score = look_ahead_cost + mst_remaining_cost + normalized_edge_cost - momentum_direction\n        scores.append(score)\n    \n    next_node = candidates[np.argmin(scores)]\n    return next_node",
          "objective": 6.44188,
          "other_inf": null,
          "metadata": {
               "operator": "i1",
               "wisdom_tips": [
                    "Design adaptive hybrid meta-heuristics synergistically fusing multiple search paradigms and dynamically tune operator parameters based on search stage or problem features",
                    "Employ machine learning or pattern recognition to mine deep problem structures and optimal solution patterns then use learned insights to intelligently bias towards promising search regions or constructive choices",
                    "Explore objective function engineering by introducing auxiliary or surrogate objectives or by dynamically adjusting weights to reshape the search landscape aiding escape from local optima or guiding diverse exploration"
               ],
               "attention_focus": "considering long-term impact of current decisions",
               "generation_tendency": "balanced_search",
               "timestamp": 1748447722.06198
          }
     },
     {
          "algorithm": "The algorithm integrates a dual-phase search strategy combining local gradient-following for immediate refinement and global pattern-driven diversification guided by learned node embeddings, adaptively switching between phases based on stagnation detection.}\n\n```python\nimport numpy as np\n\ndef select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):\n    def learn_node_embeddings(nodes, dist_matrix):\n        # Learn low-dimensional embeddings using multidimensional scaling (MDS)\n        from sklearn.manifold import MDS\n        mds = MDS(n_components=2, dissimilarity='precomputed', random_state=0)\n        embeddings = mds.fit_transform(dist_matrix[np.ix_(nodes, nodes)])\n        return {node: embeddings[i] for i, node in enumerate(nodes)}\n    \n    def local_gradient_score(candidate, current_node, dist_matrix):\n        return dist_matrix[current_node][candidate]\n    \n    def global_pattern_score(candidate, embeddings):\n        # Encourage diversity by penalizing proximity to already visited regions\n        avg_embedding = np.mean([emb for node, emb in embeddings.items() if node != candidate], axis=0)\n        return np.linalg.norm(embeddings[candidate] - avg_embedding)\n    \n    def detect_stagnation(scores, threshold=1e-3):\n        return np.std(scores) < threshold\n    \n    # Phase 1: Local refinement\n    local_scores = {cand: local_gradient_score(cand, current_node, distance_matrix) for cand in unvisited_nodes}\n    best_local_candidate = min(local_scores, key=local_scores.get)\n    \n    # Phase 2: Global diversification\n    embeddings = learn_node_embeddings(unvisited_nodes, distance_matrix)\n    global_scores = {cand: global_pattern_score(cand, embeddings) for cand in unvisited_nodes",
          "code": "import numpy as np\n\ndef select_next_node(current_node, destination_node, unvisited_nodes, distance_matrix):\n    def learn_node_embeddings(nodes, dist_matrix):\n        # Learn low-dimensional embeddings using multidimensional scaling (MDS)\n        from sklearn.manifold import MDS\n        mds = MDS(n_components=2, dissimilarity='precomputed', random_state=0)\n        embeddings = mds.fit_transform(dist_matrix[np.ix_(nodes, nodes)])\n        return {node: embeddings[i] for i, node in enumerate(nodes)}\n    \n    def local_gradient_score(candidate, current_node, dist_matrix):\n        return dist_matrix[current_node][candidate]\n    \n    def global_pattern_score(candidate, embeddings):\n        # Encourage diversity by penalizing proximity to already visited regions\n        avg_embedding = np.mean([emb for node, emb in embeddings.items() if node != candidate], axis=0)\n        return np.linalg.norm(embeddings[candidate] - avg_embedding)\n    \n    def detect_stagnation(scores, threshold=1e-3):\n        return np.std(scores) < threshold\n    \n    # Phase 1: Local refinement\n    local_scores = {cand: local_gradient_score(cand, current_node, distance_matrix) for cand in unvisited_nodes}\n    best_local_candidate = min(local_scores, key=local_scores.get)\n    \n    # Phase 2: Global diversification\n    embeddings = learn_node_embeddings(unvisited_nodes, distance_matrix)\n    global_scores = {cand: global_pattern_score(cand, embeddings) for cand in unvisited_nodes}\n    best_global_candidate = max(global_scores, key=global_scores.get)\n    \n    # Adaptive phase switching based on stagnation\n    if detect_stagnation(list(local_scores.values())):\n        next_node = best_global_candidate\n    else:\n        next_node = best_local_candidate\n    \n    return next_node",
          "objective": 6.60788,
          "other_inf": null,
          "metadata": {
               "operator": "e2",
               "wisdom_tips": [
                    "Design adaptive hybrid meta-heuristics synergistically fusing multiple search paradigms and dynamically tune operator parameters based on search stage or problem features",
                    "Employ machine learning or pattern recognition to mine deep problem structures and optimal solution patterns then use learned insights to intelligently bias towards promising search regions or constructive choices",
                    "Explore objective function engineering by introducing auxiliary or surrogate objectives or by dynamically adjusting weights to reshape the search landscape aiding escape from local optima or guiding diverse exploration"
               ],
               "attention_focus": "balancing local optimality with global search strategies",
               "generation_tendency": "balanced_search",
               "timestamp": 1748447886.251133
          }
     }
]